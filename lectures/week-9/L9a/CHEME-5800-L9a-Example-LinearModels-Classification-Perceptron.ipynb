{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e92cf5-76c0-4aba-9d0a-d5813ef680b1",
   "metadata": {},
   "source": [
    "# Activity: Linear Models for Classification\n",
    "In this task, we will implement the perceptron algorithm for a binary classification problem.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> * **Train a classifier**: You will train a perceptron classifier to distinguish between genuine and forged banknotes using real image features.\n",
    "> * **Test model performance**: You will test your trained model on unseen data to measure how well it performs on new examples.\n",
    "> * **Analyze classification errors**: You will analyze classification mistakes using confusion matrices to understand false positives and false negatives.\n",
    ">\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7086ade",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764c2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ad56c",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585183e",
   "metadata": {},
   "source": [
    "### Data\n",
    "The dataset we will explore is the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). This dataset has `1372` instances with 4 continuous features and an integer $\\{-1,1\\}$ class variable. \n",
    "\n",
    "> __Description of the dataset__ \n",
    "> \n",
    "> * Data were extracted from images taken from genuine and forged banknote-like specimens. An industrial camera, usually used for print inspection, was used for digitization. The final images have 400x400 pixels. Due to the object lens and distance to the investigated object, gray-scale pictures with a resolution of about 660 dpi were obtained. Wavelet Transform tools were used to extract features from images.\n",
    "> * __Features__: The data has four continuous features from each image: `variance` of the wavelet transformed image, `skewness` of the wavelet transformed image, `kurtosis` of the wavelet transformed image, and the `entropy` of the wavelet transformed image. The class is $\\{-1,1\\}$ where a class value of `-1` indicates genuine and `1` indicates forged.\n",
    "\n",
    "We've included this dataset in [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl) and have provided [the `MyBanknoteAuthenticationDataset(...)` helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset) for easy access. \n",
    "\n",
    "This method returns the data in [a `DataFrame` instance](https://github.com/JuliaData/DataFrames.jl), which we'll save in the `df_banknote` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a04c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banknote =  MyBanknoteAuthenticationDataset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd1817b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1372×5 DataFrame</span></div><div style = \"float: right; font-style: italic;\"><span>1347 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variance</th><th style = \"text-align: left;\">skewness</th><th style = \"text-align: left;\">curtosis</th><th style = \"text-align: left;\">entropy</th><th style = \"text-align: left;\">class</th></tr><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.6216</td><td style = \"text-align: right;\">8.6661</td><td style = \"text-align: right;\">-2.8073</td><td style = \"text-align: right;\">-0.44699</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.5459</td><td style = \"text-align: right;\">8.1674</td><td style = \"text-align: right;\">-2.4586</td><td style = \"text-align: right;\">-1.4621</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.866</td><td style = \"text-align: right;\">-2.6383</td><td style = \"text-align: right;\">1.9242</td><td style = \"text-align: right;\">0.10645</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.4566</td><td style = \"text-align: right;\">9.5228</td><td style = \"text-align: right;\">-4.0112</td><td style = \"text-align: right;\">-3.5944</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.32924</td><td style = \"text-align: right;\">-4.4552</td><td style = \"text-align: right;\">4.5718</td><td style = \"text-align: right;\">-0.9888</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.3684</td><td style = \"text-align: right;\">9.6718</td><td style = \"text-align: right;\">-3.9606</td><td style = \"text-align: right;\">-3.1625</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3.5912</td><td style = \"text-align: right;\">3.0129</td><td style = \"text-align: right;\">0.72888</td><td style = \"text-align: right;\">0.56421</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">2.0922</td><td style = \"text-align: right;\">-6.81</td><td style = \"text-align: right;\">8.4636</td><td style = \"text-align: right;\">-0.60216</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3.2032</td><td style = \"text-align: right;\">5.7588</td><td style = \"text-align: right;\">-0.75345</td><td style = \"text-align: right;\">-0.61251</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1.5356</td><td style = \"text-align: right;\">9.1772</td><td style = \"text-align: right;\">-2.2718</td><td style = \"text-align: right;\">-0.73535</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1.2247</td><td style = \"text-align: right;\">8.7779</td><td style = \"text-align: right;\">-2.2135</td><td style = \"text-align: right;\">-0.80647</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.9899</td><td style = \"text-align: right;\">-2.7066</td><td style = \"text-align: right;\">2.3946</td><td style = \"text-align: right;\">0.86291</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1.8993</td><td style = \"text-align: right;\">7.6625</td><td style = \"text-align: right;\">0.15394</td><td style = \"text-align: right;\">-3.1108</td><td style = \"text-align: right;\">-1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1361</td><td style = \"text-align: right;\">-0.24745</td><td style = \"text-align: right;\">1.9368</td><td style = \"text-align: right;\">-2.4697</td><td style = \"text-align: right;\">-0.80518</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1362</td><td style = \"text-align: right;\">-1.5732</td><td style = \"text-align: right;\">1.0636</td><td style = \"text-align: right;\">-0.71232</td><td style = \"text-align: right;\">-0.8388</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1363</td><td style = \"text-align: right;\">-2.1668</td><td style = \"text-align: right;\">1.5933</td><td style = \"text-align: right;\">0.045122</td><td style = \"text-align: right;\">-1.678</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1364</td><td style = \"text-align: right;\">-1.1667</td><td style = \"text-align: right;\">-1.4237</td><td style = \"text-align: right;\">2.9241</td><td style = \"text-align: right;\">0.66119</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1365</td><td style = \"text-align: right;\">-2.8391</td><td style = \"text-align: right;\">-6.63</td><td style = \"text-align: right;\">10.4849</td><td style = \"text-align: right;\">-0.42113</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1366</td><td style = \"text-align: right;\">-4.5046</td><td style = \"text-align: right;\">-5.8126</td><td style = \"text-align: right;\">10.8867</td><td style = \"text-align: right;\">-0.52846</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1367</td><td style = \"text-align: right;\">-2.41</td><td style = \"text-align: right;\">3.7433</td><td style = \"text-align: right;\">-0.40215</td><td style = \"text-align: right;\">-1.2953</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1368</td><td style = \"text-align: right;\">0.40614</td><td style = \"text-align: right;\">1.3492</td><td style = \"text-align: right;\">-1.4501</td><td style = \"text-align: right;\">-0.55949</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1369</td><td style = \"text-align: right;\">-1.3887</td><td style = \"text-align: right;\">-4.8773</td><td style = \"text-align: right;\">6.4774</td><td style = \"text-align: right;\">0.34179</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1370</td><td style = \"text-align: right;\">-3.7503</td><td style = \"text-align: right;\">-13.4586</td><td style = \"text-align: right;\">17.5932</td><td style = \"text-align: right;\">-2.7771</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1371</td><td style = \"text-align: right;\">-3.5637</td><td style = \"text-align: right;\">-8.3827</td><td style = \"text-align: right;\">12.393</td><td style = \"text-align: right;\">-1.2823</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1372</td><td style = \"text-align: right;\">-2.5419</td><td style = \"text-align: right;\">-0.65804</td><td style = \"text-align: right;\">2.6842</td><td style = \"text-align: right;\">1.1952</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& variance & skewness & curtosis & entropy & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.6216 & 8.6661 & -2.8073 & -0.44699 & -1 \\\\\n",
       "\t2 & 4.5459 & 8.1674 & -2.4586 & -1.4621 & -1 \\\\\n",
       "\t3 & 3.866 & -2.6383 & 1.9242 & 0.10645 & -1 \\\\\n",
       "\t4 & 3.4566 & 9.5228 & -4.0112 & -3.5944 & -1 \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t6 & 4.3684 & 9.6718 & -3.9606 & -3.1625 & -1 \\\\\n",
       "\t7 & 3.5912 & 3.0129 & 0.72888 & 0.56421 & -1 \\\\\n",
       "\t8 & 2.0922 & -6.81 & 8.4636 & -0.60216 & -1 \\\\\n",
       "\t9 & 3.2032 & 5.7588 & -0.75345 & -0.61251 & -1 \\\\\n",
       "\t10 & 1.5356 & 9.1772 & -2.2718 & -0.73535 & -1 \\\\\n",
       "\t11 & 1.2247 & 8.7779 & -2.2135 & -0.80647 & -1 \\\\\n",
       "\t12 & 3.9899 & -2.7066 & 2.3946 & 0.86291 & -1 \\\\\n",
       "\t13 & 1.8993 & 7.6625 & 0.15394 & -3.1108 & -1 \\\\\n",
       "\t14 & -1.5768 & 10.843 & 2.5462 & -2.9362 & -1 \\\\\n",
       "\t15 & 3.404 & 8.7261 & -2.9915 & -0.57242 & -1 \\\\\n",
       "\t16 & 4.6765 & -3.3895 & 3.4896 & 1.4771 & -1 \\\\\n",
       "\t17 & 2.6719 & 3.0646 & 0.37158 & 0.58619 & -1 \\\\\n",
       "\t18 & 0.80355 & 2.8473 & 4.3439 & 0.6017 & -1 \\\\\n",
       "\t19 & 1.4479 & -4.8794 & 8.3428 & -2.1086 & -1 \\\\\n",
       "\t20 & 5.2423 & 11.0272 & -4.353 & -4.1013 & -1 \\\\\n",
       "\t21 & 5.7867 & 7.8902 & -2.6196 & -0.48708 & -1 \\\\\n",
       "\t22 & 0.3292 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t23 & 3.9362 & 10.1622 & -3.8235 & -4.0172 & -1 \\\\\n",
       "\t24 & 0.93584 & 8.8855 & -1.6831 & -1.6599 & -1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1372×5 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m variance \u001b[0m\u001b[1m skewness  \u001b[0m\u001b[1m curtosis \u001b[0m\u001b[1m entropy  \u001b[0m\u001b[1m class \u001b[0m\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "──────┼────────────────────────────────────────────────\n",
       "    1 │  3.6216     8.6661   -2.8073   -0.44699     -1\n",
       "    2 │  4.5459     8.1674   -2.4586   -1.4621      -1\n",
       "    3 │  3.866     -2.6383    1.9242    0.10645     -1\n",
       "    4 │  3.4566     9.5228   -4.0112   -3.5944      -1\n",
       "    5 │  0.32924   -4.4552    4.5718   -0.9888      -1\n",
       "    6 │  4.3684     9.6718   -3.9606   -3.1625      -1\n",
       "    7 │  3.5912     3.0129    0.72888   0.56421     -1\n",
       "    8 │  2.0922    -6.81      8.4636   -0.60216     -1\n",
       "  ⋮   │    ⋮          ⋮         ⋮         ⋮        ⋮\n",
       " 1366 │ -4.5046    -5.8126   10.8867   -0.52846      1\n",
       " 1367 │ -2.41       3.7433   -0.40215  -1.2953       1\n",
       " 1368 │  0.40614    1.3492   -1.4501   -0.55949      1\n",
       " 1369 │ -1.3887    -4.8773    6.4774    0.34179      1\n",
       " 1370 │ -3.7503   -13.4586   17.5932   -2.7771       1\n",
       " 1371 │ -3.5637    -8.3827   12.393    -1.2823       1\n",
       " 1372 │ -2.5419    -0.65804   2.6842    1.1952       1\n",
       "\u001b[36m                                      1357 rows omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_banknote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85555fa5",
   "metadata": {},
   "source": [
    "Now let's split the dataset into the system input matrix $\\mathbf{X}$ (independent variables, characteristics of the banknote) and the output vector $\\mathbf{y}$ (dependent variable, the banknote class).\n",
    "\n",
    "The input matrix $\\mathbf{X}$ will contain all the columns except for the `class` column (the output variable). The output vector $\\mathbf{y}$ will contain only the `class` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3ba885",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(df_banknote[:, Not(:class)]); # data matrix: select all the columns *except* class\n",
    "y = Vector(df_banknote[:, :class]); # output vector: select the class column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668bf03",
   "metadata": {},
   "source": [
    "Finally, let's partition the data into a `training` and `testing` set so that we can determine how well the model can predict unseen data, i.e., how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53781a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = let\n",
    "\n",
    "    # initialize -\n",
    "    s = 0.80; # fraction of data for training\n",
    "    number_of_training_samples = Int(round(s * size(X,1))); # 80% of the data for training\n",
    "    i = randperm(size(X,1)); # random permutation of the indices\n",
    "    training_indices = i[1:number_of_training_samples]; # first 80% of the indices\n",
    "    testing_indices = i[number_of_training_samples+1:end]; # last 20% of\n",
    "    \n",
    "\n",
    "    # setup training -\n",
    "    one_vector = ones(number_of_training_samples);\n",
    "    training = (X=[X[training_indices, :] one_vector], y=y[training_indices]);\n",
    "\n",
    "    # setup testing -\n",
    "    one_vector = ones(length(testing_indices));\n",
    "    testing = (X=[X[testing_indices, :] one_vector], y=y[testing_indices]);\n",
    "\n",
    "    training, testing;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e4d9a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ad6fb",
   "metadata": {},
   "source": [
    "## Implement the Perceptron Algorithm\n",
    "In this task, we will implement the perceptron algorithm for a binary classification problem using the banknote authentication dataset. We'll use the online learning version of the perceptron algorithm.\n",
    "\n",
    "Let's examine the pseudocode for the Perceptron learning algorithm. \n",
    "\n",
    "__Initialize__: Given a linearly separable dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$, the maximum number of iterations $T$, and the maximum number of mistakes $M$ (e.g., $M=1$), initialize the parameter vector $\\theta = \\left(\\mathbf{w}, b\\right)$ to small random values and set the loop counter $t\\gets{0}$.\n",
    "\n",
    "> **Rule of thumb for $T$**: Set $T = 10n$ to $100n$, where $n$ is the number of training examples. The algorithm often converges faster for linearly separable data.\n",
    "\n",
    "While $\\texttt{true}$ __do__:\n",
    "1. Initialize the number of mistakes $\\texttt{mistakes} = 0$.\n",
    "2. For each training example $(\\mathbf{x}, y) \\in \\mathcal{D}$: compute $y\\;\\left(\\theta^{\\top}\\;\\mathbf{x}\\right)\\leq{0}$. If this condition is $\\texttt{true}$, then the training example $(\\mathbf{x}, y)$ is misclassified. Update the parameter vector $\\theta \\gets \\theta + y\\;\\mathbf{x}$ and increment the error counter $\\texttt{mistakes} \\gets \\texttt{mistakes} + 1$.\n",
    "3. After processing all training examples, if $\\texttt{mistakes} \\leq {M}$ or $t \\geq T$, break the loop. Otherwise, increment the loop counter $t \\gets t + 1$ and repeat from step 1.\n",
    "\n",
    "__Hypothesis__: If the banknote dataset $\\mathcal{D}$ is linearly separable, the Perceptron will _incrementally_ learn a separating hyperplane in a finite number of passes through the dataset $\\mathcal{D}$. However, if the dataset $\\mathcal{D}$ is not linearly separable, the Perceptron may not converge. Check out the [perceptron pseudocode here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf)\n",
    "\n",
    "__Training__: Our Perceptron implementation [based on pseudocode](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/#VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel) required by the problem. We then _learn_ the model parameters [using the `learn(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.learn), which takes the (augmented) training features array `X`, the training labels vector `y`, and the problem instance and returns an updated problem instance holding the updated parameters.\n",
    "\n",
    "We save the trained classifier in the `model::MyPerceptronClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7286516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1000. We have number of errors: 12\n"
     ]
    }
   ],
   "source": [
    "model = let\n",
    "\n",
    "    # data -\n",
    "    X = training.X; # input matrix\n",
    "    y = training.y; # output vector\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "\n",
    "    # model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f84d4",
   "metadata": {},
   "source": [
    "Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. \n",
    "> __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) labels in the `y_banknote::Array{Int64,1}` vector, while the model predicted labels are stored in the `ŷ_banknote::Array{Int64,1}` array.\n",
    "\n",
    "Let's run the classifier on the `testing` data and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1848cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_banknote,y_banknote = let\n",
    "\n",
    "    X = testing.X; # what dataset are going to use?\n",
    "    y = testing.y; # what are the actual labels?\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4908e",
   "metadata": {},
   "source": [
    "How many mistakes did the classifier make on the `testing` dataset? Let's count the number of times $\\hat{y}_{i}\\neq{y}_{i}$, i.e., when the inference predicts the wrong label.\n",
    "> __Note__: Does having the total error count tell us the whole story? It would be helpful to know how many were false positives and how many were false negatives. Let's compute those values as well. Let's start with the total number of errors.\n",
    "\n",
    "Let's store the total number of errors in the `number_of_prediction_mistakes::Int64` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382b0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_mistakes = let\n",
    "\n",
    "    number_of_test_examples = length(ŷ_banknote);\n",
    "    error_counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (ŷ_banknote[i] != y_banknote[i])\n",
    "            error_counter += 1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    error_counter\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1905ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron mistake percentage: 1.094890510948905%\n"
     ]
    }
   ],
   "source": [
    "println(\"Perceptron mistake percentage: $((number_of_prediction_mistakes/length(ŷ_banknote))*100)%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3d88e",
   "metadata": {},
   "source": [
    "If we were predicting a continuous variable, we could compute the residuals of the predictions, but since we are predicting a categorical variable, we can only count the number of times we predicted the wrong label.\n",
    "\n",
    "> __Error analysis__: Knowing the total number of mistakes is only part of the story. It is useful to know if we are biased towards false positives or false negatives, that is, how many times did we predict a banknote was a forgery when it was genuine (false positive), and how many times did we predict a banknote was genuine when it was a forgery (false negative).\n",
    "\n",
    "We can get the false positive and false negative counts by computing the __confusion matrix__. We've implemented a helper function, [the `confusion(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion), that computes the confusion matrix for us. This method takes the actual labels vector `y` and the estimated labels vector `ŷ` and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c429d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion(y_banknote,ŷ_banknote); # important: actual labels first, estimated labels second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0906060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 112    2\n",
       "   1  159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97c7c6",
   "metadata": {},
   "source": [
    "According to the confusion matrix, how many false positives and false negatives did we have? We have `2` false positives and `1` false negative.\n",
    "\n",
    "> __Note__: A perfect classifier would have `0` false positives and `0` false negatives. The results we reported here may vary slightly each time you run the notebook because of the random partitioning of the data into `training` and `testing` sets.\n",
    "\n",
    "Not bad for a simple linear classifier! We could take this further by computing other metrics such as accuracy, precision, recall, and specificity from the confusion matrix. However, we'll leave that for another time.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23cd21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
