{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e92cf5-76c0-4aba-9d0a-d5813ef680b1",
   "metadata": {},
   "source": [
    "# Example: Linear Models for Classification\n",
    "In this example, we implement the Perceptron algorithm for a binary classification problem.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> * **Train a classifier**: Implement the Perceptron algorithm and understand how it learns a linear decision boundary. Train a classifier to distinguish between genuine and forged banknotes using image features, and observe how the algorithm updates parameters with each pass through the data.\n",
    "> * **Test model performance**: Evaluate your trained model on unseen test data to measure how well it generalizes. Understand the difference between training error and test error, and why we evaluate on data the model has never seen.\n",
    "> * **Analyze classification errors**: Use confusion matrices to break down classification mistakes into false positives and false negatives. Interpret what these errors mean for your specific problem and understand the trade-offs between different types of errors.\n",
    ">\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7086ade",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading required resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the input source file `Include.jl` in the notebook's global scope. This file sets paths, loads packages, and more. For more information on functions and types, see the [Julia documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up the code environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764c2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ad56c",
   "metadata": {},
   "source": [
    "We also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). See [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for details on functions, types, and data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585183e",
   "metadata": {},
   "source": [
    "### Data\n",
    "We use the [banknote authentication dataset from UCI](https://archive.ics.uci.edu/dataset/267/banknote+authentication), which has 1372 instances with 4 continuous features and a $\\{-1,1\\}$ class variable. \n",
    "\n",
    "> __Dataset Description__ \n",
    "> \n",
    "> * Images of genuine and forged banknote specimens were captured using an industrial camera. Final images are 400x400 pixels. Gray-scale images with ~660 dpi resolution were obtained. Wavelet Transform tools extracted features.\n",
    "> * __Features__: Four continuous features from each image: `variance` of the wavelet transform, `skewness` of the wavelet transform, `kurtosis` of the wavelet transform, and `entropy` of the wavelet transform. The class is $\\{-1,1\\}$ where $-1$ indicates genuine and $1$ indicates forged.\n",
    "\n",
    "The [VLDataScienceMachineLearningPackage.jl](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl) includes this dataset. We use the [MyBanknoteAuthenticationDataset(...) helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset) for easy access. \n",
    "\n",
    "This method returns the data in a [DataFrame](https://github.com/JuliaData/DataFrames.jl), which we save in the `df_banknote` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a04c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banknote =  MyBanknoteAuthenticationDataset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd1817b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1372×5 DataFrame</span></div><div style = \"float: right; font-style: italic;\"><span>1347 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variance</th><th style = \"text-align: left;\">skewness</th><th style = \"text-align: left;\">curtosis</th><th style = \"text-align: left;\">entropy</th><th style = \"text-align: left;\">class</th></tr><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.6216</td><td style = \"text-align: right;\">8.6661</td><td style = \"text-align: right;\">-2.8073</td><td style = \"text-align: right;\">-0.44699</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.5459</td><td style = \"text-align: right;\">8.1674</td><td style = \"text-align: right;\">-2.4586</td><td style = \"text-align: right;\">-1.4621</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.866</td><td style = \"text-align: right;\">-2.6383</td><td style = \"text-align: right;\">1.9242</td><td style = \"text-align: right;\">0.10645</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.4566</td><td style = \"text-align: right;\">9.5228</td><td style = \"text-align: right;\">-4.0112</td><td style = \"text-align: right;\">-3.5944</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.32924</td><td style = \"text-align: right;\">-4.4552</td><td style = \"text-align: right;\">4.5718</td><td style = \"text-align: right;\">-0.9888</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.3684</td><td style = \"text-align: right;\">9.6718</td><td style = \"text-align: right;\">-3.9606</td><td style = \"text-align: right;\">-3.1625</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3.5912</td><td style = \"text-align: right;\">3.0129</td><td style = \"text-align: right;\">0.72888</td><td style = \"text-align: right;\">0.56421</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">2.0922</td><td style = \"text-align: right;\">-6.81</td><td style = \"text-align: right;\">8.4636</td><td style = \"text-align: right;\">-0.60216</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3.2032</td><td style = \"text-align: right;\">5.7588</td><td style = \"text-align: right;\">-0.75345</td><td style = \"text-align: right;\">-0.61251</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1.5356</td><td style = \"text-align: right;\">9.1772</td><td style = \"text-align: right;\">-2.2718</td><td style = \"text-align: right;\">-0.73535</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1.2247</td><td style = \"text-align: right;\">8.7779</td><td style = \"text-align: right;\">-2.2135</td><td style = \"text-align: right;\">-0.80647</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.9899</td><td style = \"text-align: right;\">-2.7066</td><td style = \"text-align: right;\">2.3946</td><td style = \"text-align: right;\">0.86291</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1.8993</td><td style = \"text-align: right;\">7.6625</td><td style = \"text-align: right;\">0.15394</td><td style = \"text-align: right;\">-3.1108</td><td style = \"text-align: right;\">-1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1361</td><td style = \"text-align: right;\">-0.24745</td><td style = \"text-align: right;\">1.9368</td><td style = \"text-align: right;\">-2.4697</td><td style = \"text-align: right;\">-0.80518</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1362</td><td style = \"text-align: right;\">-1.5732</td><td style = \"text-align: right;\">1.0636</td><td style = \"text-align: right;\">-0.71232</td><td style = \"text-align: right;\">-0.8388</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1363</td><td style = \"text-align: right;\">-2.1668</td><td style = \"text-align: right;\">1.5933</td><td style = \"text-align: right;\">0.045122</td><td style = \"text-align: right;\">-1.678</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1364</td><td style = \"text-align: right;\">-1.1667</td><td style = \"text-align: right;\">-1.4237</td><td style = \"text-align: right;\">2.9241</td><td style = \"text-align: right;\">0.66119</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1365</td><td style = \"text-align: right;\">-2.8391</td><td style = \"text-align: right;\">-6.63</td><td style = \"text-align: right;\">10.4849</td><td style = \"text-align: right;\">-0.42113</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1366</td><td style = \"text-align: right;\">-4.5046</td><td style = \"text-align: right;\">-5.8126</td><td style = \"text-align: right;\">10.8867</td><td style = \"text-align: right;\">-0.52846</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1367</td><td style = \"text-align: right;\">-2.41</td><td style = \"text-align: right;\">3.7433</td><td style = \"text-align: right;\">-0.40215</td><td style = \"text-align: right;\">-1.2953</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1368</td><td style = \"text-align: right;\">0.40614</td><td style = \"text-align: right;\">1.3492</td><td style = \"text-align: right;\">-1.4501</td><td style = \"text-align: right;\">-0.55949</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1369</td><td style = \"text-align: right;\">-1.3887</td><td style = \"text-align: right;\">-4.8773</td><td style = \"text-align: right;\">6.4774</td><td style = \"text-align: right;\">0.34179</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1370</td><td style = \"text-align: right;\">-3.7503</td><td style = \"text-align: right;\">-13.4586</td><td style = \"text-align: right;\">17.5932</td><td style = \"text-align: right;\">-2.7771</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1371</td><td style = \"text-align: right;\">-3.5637</td><td style = \"text-align: right;\">-8.3827</td><td style = \"text-align: right;\">12.393</td><td style = \"text-align: right;\">-1.2823</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1372</td><td style = \"text-align: right;\">-2.5419</td><td style = \"text-align: right;\">-0.65804</td><td style = \"text-align: right;\">2.6842</td><td style = \"text-align: right;\">1.1952</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& variance & skewness & curtosis & entropy & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.6216 & 8.6661 & -2.8073 & -0.44699 & -1 \\\\\n",
       "\t2 & 4.5459 & 8.1674 & -2.4586 & -1.4621 & -1 \\\\\n",
       "\t3 & 3.866 & -2.6383 & 1.9242 & 0.10645 & -1 \\\\\n",
       "\t4 & 3.4566 & 9.5228 & -4.0112 & -3.5944 & -1 \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t6 & 4.3684 & 9.6718 & -3.9606 & -3.1625 & -1 \\\\\n",
       "\t7 & 3.5912 & 3.0129 & 0.72888 & 0.56421 & -1 \\\\\n",
       "\t8 & 2.0922 & -6.81 & 8.4636 & -0.60216 & -1 \\\\\n",
       "\t9 & 3.2032 & 5.7588 & -0.75345 & -0.61251 & -1 \\\\\n",
       "\t10 & 1.5356 & 9.1772 & -2.2718 & -0.73535 & -1 \\\\\n",
       "\t11 & 1.2247 & 8.7779 & -2.2135 & -0.80647 & -1 \\\\\n",
       "\t12 & 3.9899 & -2.7066 & 2.3946 & 0.86291 & -1 \\\\\n",
       "\t13 & 1.8993 & 7.6625 & 0.15394 & -3.1108 & -1 \\\\\n",
       "\t14 & -1.5768 & 10.843 & 2.5462 & -2.9362 & -1 \\\\\n",
       "\t15 & 3.404 & 8.7261 & -2.9915 & -0.57242 & -1 \\\\\n",
       "\t16 & 4.6765 & -3.3895 & 3.4896 & 1.4771 & -1 \\\\\n",
       "\t17 & 2.6719 & 3.0646 & 0.37158 & 0.58619 & -1 \\\\\n",
       "\t18 & 0.80355 & 2.8473 & 4.3439 & 0.6017 & -1 \\\\\n",
       "\t19 & 1.4479 & -4.8794 & 8.3428 & -2.1086 & -1 \\\\\n",
       "\t20 & 5.2423 & 11.0272 & -4.353 & -4.1013 & -1 \\\\\n",
       "\t21 & 5.7867 & 7.8902 & -2.6196 & -0.48708 & -1 \\\\\n",
       "\t22 & 0.3292 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t23 & 3.9362 & 10.1622 & -3.8235 & -4.0172 & -1 \\\\\n",
       "\t24 & 0.93584 & 8.8855 & -1.6831 & -1.6599 & -1 \\\\\n",
       "\t25 & 4.4338 & 9.887 & -4.6795 & -3.7483 & -1 \\\\\n",
       "\t26 & 0.7057 & -5.4981 & 8.3368 & -2.8715 & -1 \\\\\n",
       "\t27 & 1.1432 & -3.7413 & 5.5777 & -0.63578 & -1 \\\\\n",
       "\t28 & -0.38214 & 8.3909 & 2.1624 & -3.7405 & -1 \\\\\n",
       "\t29 & 6.5633 & 9.8187 & -4.4113 & -3.2258 & -1 \\\\\n",
       "\t30 & 4.8906 & -3.3584 & 3.4202 & 1.0905 & -1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1372×5 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m variance \u001b[0m\u001b[1m skewness  \u001b[0m\u001b[1m curtosis  \u001b[0m\u001b[1m entropy  \u001b[0m\u001b[1m class \u001b[0m\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "──────┼─────────────────────────────────────────────────\n",
       "    1 │  3.6216     8.6661   -2.8073    -0.44699     -1\n",
       "    2 │  4.5459     8.1674   -2.4586    -1.4621      -1\n",
       "    3 │  3.866     -2.6383    1.9242     0.10645     -1\n",
       "    4 │  3.4566     9.5228   -4.0112    -3.5944      -1\n",
       "    5 │  0.32924   -4.4552    4.5718    -0.9888      -1\n",
       "    6 │  4.3684     9.6718   -3.9606    -3.1625      -1\n",
       "    7 │  3.5912     3.0129    0.72888    0.56421     -1\n",
       "    8 │  2.0922    -6.81      8.4636    -0.60216     -1\n",
       "    9 │  3.2032     5.7588   -0.75345   -0.61251     -1\n",
       "   10 │  1.5356     9.1772   -2.2718    -0.73535     -1\n",
       "   11 │  1.2247     8.7779   -2.2135    -0.80647     -1\n",
       "  ⋮   │    ⋮          ⋮          ⋮         ⋮        ⋮\n",
       " 1363 │ -2.1668     1.5933    0.045122  -1.678        1\n",
       " 1364 │ -1.1667    -1.4237    2.9241     0.66119      1\n",
       " 1365 │ -2.8391    -6.63     10.4849    -0.42113      1\n",
       " 1366 │ -4.5046    -5.8126   10.8867    -0.52846      1\n",
       " 1367 │ -2.41       3.7433   -0.40215   -1.2953       1\n",
       " 1368 │  0.40614    1.3492   -1.4501    -0.55949      1\n",
       " 1369 │ -1.3887    -4.8773    6.4774     0.34179      1\n",
       " 1370 │ -3.7503   -13.4586   17.5932    -2.7771       1\n",
       " 1371 │ -3.5637    -8.3827   12.393     -1.2823       1\n",
       " 1372 │ -2.5419    -0.65804   2.6842     1.1952       1\n",
       "\u001b[36m                                       1351 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_banknote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85555fa5",
   "metadata": {},
   "source": [
    "Now we split the dataset into the input matrix $\\mathbf{X}$ (independent variables, banknote features) and output vector $\\mathbf{y}$ (dependent variable, banknote class).\n",
    "\n",
    "We create $\\mathbf{X}$ with all columns except the `class` column, and $\\mathbf{y}$ with only the `class` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3ba885",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(df_banknote[:, Not(:class)]); # data matrix: select all the columns *except* class\n",
    "y = Vector(df_banknote[:, :class]); # output vector: select the class column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668bf03",
   "metadata": {},
   "source": [
    "Finally, we partition the data into `training` and `testing` sets. This allows us to measure how well the model generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53781a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = let\n",
    "\n",
    "    # initialize -\n",
    "    s = 0.80; # fraction of data for training\n",
    "    number_of_training_samples = Int(round(s * size(X,1))); # 80% of the data for training\n",
    "    i = randperm(size(X,1)); # random permutation of the indices\n",
    "    training_indices = i[1:number_of_training_samples]; # first 80% of the indices\n",
    "    testing_indices = i[number_of_training_samples+1:end]; # last 20% of\n",
    "    \n",
    "\n",
    "    # setup training -\n",
    "    one_vector = ones(number_of_training_samples);\n",
    "    training = (X=[X[training_indices, :] one_vector], y=y[training_indices]);\n",
    "\n",
    "    # setup testing -\n",
    "    one_vector = ones(length(testing_indices));\n",
    "    testing = (X=[X[testing_indices, :] one_vector], y=y[testing_indices]);\n",
    "\n",
    "    training, testing;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52337560-0ddc-4c2a-bd44-99122e85a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098-element Vector{Int64}:\n",
       "  1\n",
       "  1\n",
       " -1\n",
       "  1\n",
       " -1\n",
       " -1\n",
       " -1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " -1\n",
       " -1\n",
       "  1\n",
       "  ⋮\n",
       " -1\n",
       " -1\n",
       "  1\n",
       "  1\n",
       " -1\n",
       " -1\n",
       " -1\n",
       " -1\n",
       "  1\n",
       " -1\n",
       "  1\n",
       "  1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e4d9a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ad6fb",
   "metadata": {},
   "source": [
    "## Implement the Perceptron Algorithm\n",
    "We implement the Perceptron algorithm for the banknote dataset using the online learning version.\n",
    "\n",
    "Here is the Perceptron learning algorithm pseudocode: \n",
    "\n",
    "__Initialize__: Given a linearly separable dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$, maximum iterations $T$, and maximum mistakes $M$ (e.g., $M=1$), initialize parameter vector $\\theta = \\left(\\mathbf{w}, b\\right)$ to small random values and set loop counter $t\\gets{0}$.\n",
    "\n",
    "> **Rule of thumb for $T$**: Set $T = 10n$ to $100n$, where $n$ is the number of training examples. Convergence is usually faster for linearly separable data.\n",
    "\n",
    "While $\\texttt{true}$ __do__:\n",
    "1. Initialize mistakes $\\texttt{mistakes} = 0$.\n",
    "2. For each training example $(\\mathbf{x}, y) \\in \\mathcal{D}$: compute $y\\;\\left(\\theta^{\\top}\\;\\mathbf{x}\\right)\\leq{0}$. If true, the example is misclassified. Update $\\theta \\gets \\theta + y\\;\\mathbf{x}$ and increment $\\texttt{mistakes} \\gets \\texttt{mistakes} + 1$.\n",
    "3. After processing all examples, if $\\texttt{mistakes} \\leq {M}$ or $t \\geq T$, exit. Otherwise, increment $t \\gets t + 1$ and repeat from step 1.\n",
    "\n",
    "__Convergence__: If the dataset $\\mathcal{D}$ is linearly separable, the Perceptron converges to a separating hyperplane in finite iterations. If not linearly separable, the Perceptron may not converge. See the [Perceptron pseudocode](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) for details.\n",
    "\n",
    "__Training__: Our Perceptron implementation stores problem data in a [MyPerceptronClassificationModel instance](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/#VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel). We learn parameters using the [learn(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.learn), which takes the feature array `X`, labels vector `y`, and problem instance, returning an updated instance with the learned parameters.\n",
    "\n",
    "The trained classifier is stored in the `model` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7286516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1000. We have number of errors: 12\n"
     ]
    }
   ],
   "source": [
    "model = let\n",
    "\n",
    "    # data -\n",
    "    X = training.X; # input matrix\n",
    "    y = training.y; # output vector\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "\n",
    "    # model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4785c204-43ca-45f8-9a71-db296ca56c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:β, :mistakes)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(model) |> T-> fieldnames(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "006d5fd8-bff7-4327-a6dc-24b84986547d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mistakes # show we store the number of mistakes???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f84d4",
   "metadata": {},
   "source": [
    "Now we evaluate the trained model on unseen test data.\n",
    "\n",
    "> __Inference__: We run classification on test data using the [classify(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify). This takes the feature array `X` and trained model, returning estimated labels. We store actual labels in `y_banknote` and predicted labels in `ŷ_banknote`.\n",
    "\n",
    "Let's evaluate the classifier on test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1848cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_banknote,y_banknote = let\n",
    "\n",
    "    X = testing.X; # what dataset are going to use?\n",
    "    y = testing.y; # what are the actual labels?\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4908e",
   "metadata": {},
   "source": [
    "How many mistakes did the classifier make on the test dataset? We count the number of times $\\hat{y}_{i}\\neq{y}_{i}$, when predictions are wrong.\n",
    "\n",
    "> __Note__: Total error count alone doesn't tell the whole story. We also need to know how many were false positives and false negatives. Let's compute these values.\n",
    "\n",
    "We store the total number of errors in the `number_of_prediction_mistakes` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "382b0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_mistakes = let\n",
    "\n",
    "    number_of_test_examples = length(ŷ_banknote);\n",
    "    error_counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (ŷ_banknote[i] != y_banknote[i])\n",
    "            error_counter += 1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    error_counter\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d1905ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron mistake percentage: 1.094890510948905%\n"
     ]
    }
   ],
   "source": [
    "println(\"Perceptron mistake percentage: $((number_of_prediction_mistakes/length(ŷ_banknote))*100)%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3d88e",
   "metadata": {},
   "source": [
    "For categorical predictions, we can only count wrong labels, not compute residuals like with continuous predictions.\n",
    "\n",
    "> __Error analysis__: Total mistakes is only part of the story. We should understand whether we are biased toward false positives or false negatives. How many times did we predict a banknote was forged when it was genuine (false positive)? How many times did we predict genuine when it was forged (false negative)?\n",
    "\n",
    "We compute the __confusion matrix__ to get these counts. We use the [confusion(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion), which takes actual labels and estimated labels, returning the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9c429d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion(y_banknote,ŷ_banknote); # important: actual labels first, estimated labels second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0906060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 112    2\n",
       "   1  159"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97c7c6",
   "metadata": {},
   "source": [
    "According to the confusion matrix, we have 2 false positives and 1 false negative.\n",
    "\n",
    "> __Note__: A perfect classifier has 0 false positives and 0 false negatives. Results may vary slightly each run due to random data partitioning.\n",
    "\n",
    "This is good performance for a simple linear classifier! We could compute additional metrics (accuracy, precision, recall, specificity) from the confusion matrix, but we'll leave that for now.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5868f7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this activity, we implemented the Perceptron algorithm to classify genuine and forged banknotes:\n",
    "\n",
    "> __Key takeaways:__\n",
    ">\n",
    "> 1. **Training data preparation**: We split the banknote dataset into training (80%) and testing (20%) sets to evaluate generalization. The input features $\\mathbf{X}$ included four wavelet-based measurements, and labels $\\mathbf{y}$ were $\\{-1,1\\}$ for genuine and forged.\n",
    "> 2. **Perceptron learning**: We trained the model using the online Perceptron algorithm, which incrementally updates parameters when misclassifications occur. The algorithm converged in a finite number of passes through the training data.\n",
    "> 3. **Performance evaluation**: On test data, the classifier made errors we analyzed using a confusion matrix. The matrix showed true positives, false positives, false negatives, and true negatives—enabling detailed error analysis beyond simple accuracy.\n",
    "\n",
    "The Perceptron provides a straightforward approach to linear classification. Understanding its behavior and limitations prepares you for more advanced methods that handle complex, non-linearly separable data.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
