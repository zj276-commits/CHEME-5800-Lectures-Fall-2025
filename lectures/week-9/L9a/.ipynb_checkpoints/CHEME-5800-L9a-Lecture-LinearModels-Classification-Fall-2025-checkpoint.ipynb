{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a74ce1d-7f79-4233-93da-4c5a1e01b9d6",
   "metadata": {},
   "source": [
    "# L9a: Linear models for classification tasks\n",
    "In this lecture, let's look at another use case for linear models: classification tasks. Linear regression can be adapted for classification tasks by transforming the continuous output of the linear regression model into a class designation.\n",
    "\n",
    "> __Learning Objectives__\n",
    ">\n",
    "> By the end of this lecture, students will be able to:\n",
    "> - **Adapt linear models for binary classification** by understanding how continuous regression outputs can be transformed into discrete class labels through activation functions like the sign function, enabling the application of linear techniques to classification problems.\n",
    "> - **Implement and analyze the Perceptron algorithm** for learning linear decision boundaries through incremental weight updates, understanding convergence guarantees for linearly separable data and recognizing when datasets require alternative approaches.\n",
    "> - **Evaluate classifier performance using confusion matrices** to compute key metrics (accuracy, precision, recall, specificity) and interpret trade-offs between different types of classification errors in real-world applications.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77693c",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Today we will use the following example to illustrate key concepts:\n",
    "\n",
    "> [▶ Build a Perceptron classifier](CHEME-5800-L9a-Example-LinearModels-Classification-Perceptron-Fall-2025.ipynb). Implement the Perceptron algorithm to classify data into two categories based on features. This exercise demonstrates how to apply linear models to classification tasks.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edf071",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <center>\n",
    "        <img src=\"figs/Fig-LinearlySeparable-Schematic.svg\" width=\"480\"/>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105c2fa",
   "metadata": {},
   "source": [
    "## Binary Classification Problem\n",
    "Linear regression can be adapted for classification by transforming the continuous output into a class designation. We can do this in two ways: directly to a class designation or to a probability using an __activation function__ $\\sigma:\\mathbb{R}\\rightarrow{\\mathbb{R}}$.\n",
    "\n",
    "Let's examine two binary classification strategies:\n",
    "\n",
    "* [The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) is an algorithm for binary classification that learns a linear decision boundary separating two classes. The Perceptron maps continuous output to a class such as $\\sigma:\\mathbb{R}\\rightarrow\\{-1,+1\\}$ using $\\sigma(\\star) = \\text{sign}(\\star)$.\n",
    "* [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression#) uses the [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to transform linear regression output into a probability. This approach is effective for various applications. We'll explore logistic regression in the next module.\n",
    "\n",
    "Today, we focus on __the Perceptron__.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd41e5",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "[The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) transforms the output of a linear regression model $y_{i}\\in\\mathbb{R}$ using an activation function to produce discrete class values. For binary classification, we use $\\sigma:\\mathbb{R}\\rightarrow\\{-1,1\\}$. \n",
    "\n",
    "Suppose we have a labeled dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ examples, where each example is labeled by an expert in a category $y_{i}\\in\\{-1,1\\}$ with an $m$-dimensional feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$. \n",
    "\n",
    "[The Perceptron](https://en.wikipedia.org/wiki/Perceptron) learns a linear decision boundary between two classes by repeatedly processing the data. During each pass, the parameter vector $\\mathbf{\\theta}$ is updated until the number of misclassifications meets a specified threshold. \n",
    "\n",
    "[The Perceptron](https://en.wikipedia.org/wiki/Perceptron) computes the estimated label $\\hat{y}_{i}$ for feature vector $\\hat{\\mathbf{x}}_{i}$ using the $\\texttt{sign}:\\mathbb{R}\\to\\{-1,1\\}$ function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\hat{y}_{i} = \\texttt{sign}\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\;\\mathbf{\\theta}\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{\\theta}=\\left(w_{1},\\dots,w_{m}, b\\right)$ is a column vector of classifier parameters, with $w_{j}\\in\\mathbb{R}$ representing the importance of feature $j$ and $b\\in\\mathbb{R}$ a bias term. The augmented features $\\hat{\\mathbf{x}}^{\\top}_{i}=\\left(x^{(i)}_{1},\\dots,x^{(i)}_{m}, 1\\right)$ are $p = m+1$-dimensional vectors, and $\\texttt{sign}(z)$ is defined as:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\texttt{sign}(z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if}~z\\geq{0}\\\\\n",
    "        -1 & \\text{if}~z<0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "### Classical: Online Perceptron Training\n",
    "__Convergence guarantee__: If dataset $\\mathcal{D}$ is linearly separable, the Perceptron converges to a separating hyperplane in finite iterations. However, if $\\mathcal{D}$ is not linearly separable, the Perceptron may not converge. \n",
    "\n",
    "Let's examine the Perceptron learning algorithm pseudocode. \n",
    "\n",
    "__Initialize__: Given a linearly separable dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$, the maximum number of iterations $T$, and the maximum number of mistakes $M$ (e.g., $M=1$), initialize the parameter vector $\\mathbf{\\theta} = \\left(\\mathbf{w}, b\\right)$ to small random values and set the loop counter $t\\gets{0}$.\n",
    "\n",
    "> **Rule of thumb for $T$**: Set $T = 10n$ to $100n$, where $n$ is the number of training examples. The algorithm often converges faster for linearly separable data.\n",
    "\n",
    "While $\\texttt{true}$ __do__:\n",
    "1. Initialize the number of mistakes $\\texttt{mistakes} = 0$.\n",
    "2. For each training example $(\\mathbf{x}, y) \\in \\mathcal{D}$: compute $y\\;\\left(\\mathbf{\\theta}^{\\top}\\;\\mathbf{x}\\right)\\leq{0}$. \n",
    "    - If true: the example is __misclassified__ (the sign of the prediction doesn't match the label $y$). Update $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + y\\;\\mathbf{x}$ and increment $\\texttt{mistakes} \\gets \\texttt{mistakes} + 1$.\n",
    "3. After processing all examples, if $\\texttt{mistakes} \\leq {M}$ or $t \\geq T$, exit. Otherwise, increment $t \\gets t + 1$ and repeat from step 1.\n",
    "\n",
    "We aim to minimize mistakes, with $M = 0$ being ideal. However, zero mistakes may not be achievable for weakly separable or non-separable data.\n",
    "\n",
    "### Modern: Nonlinear Optimization\n",
    "In future lectures, we'll revisit Perceptron training using optimization techniques that minimize a nonlinear loss function. However, the classical online Perceptron is a good starting point for understanding linear binary classification.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d05a0e",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Binary Classification\n",
    "A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) evaluates binary classifier performance by comparing predicted labels to true labels. It provides a detailed breakdown of correct and incorrect predictions.\n",
    "\n",
    "> __Confusion Matrix Structure__\n",
    ">\n",
    "> The confusion matrix for a binary classifier is a $2\\times{2}$ table:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    ">\n",
    "> * **True Positive (TP)**: Correctly predicts positive. Example: diagnosing a disease when the patient has it.\n",
    "> * **False Positive (FP)**: Incorrectly predicts positive when negative. This is a false alarm, like telling a healthy patient they have a disease.\n",
    "> * **False Negative (FN)**: Incorrectly predicts negative when positive. This misses a true case, like failing to diagnose a disease.\n",
    "> * **True Negative (TN)**: Correctly predicts negative. Example: identifying a healthy patient as healthy.\n",
    "\n",
    "### Why the Confusion Matrix Matters\n",
    "The confusion matrix enables calculation of key performance metrics:\n",
    "\n",
    "* **Accuracy** $\\frac{TP + TN}{TP + TN + FP + FN}$: What fraction of all predictions were correct?\n",
    "* **Precision** $\\frac{TP}{TP + FP}$: Of all positive predictions, how many were correct? High precision means fewer false alarms.\n",
    "* **Recall (Sensitivity)** $\\frac{TP}{TP + FN}$: Of all actual positive cases, how many did the model identify? High recall means fewer missed cases.\n",
    "* **Specificity** $\\frac{TN}{TN + FP}$: Of all actual negative cases, how many did the model correctly identify?\n",
    "\n",
    "> **Key insight**: Different applications require different trade-offs. In medical diagnosis, high recall (avoiding missed diagnoses) may be more important than high precision.\n",
    "\n",
    "Each quadrant shows the types of errors your model makes, enabling informed decisions about improvements.\n",
    "\n",
    "Let's examine an example applying the Perceptron algorithm and confusion matrix.\n",
    "\n",
    "> __Example__\n",
    "> \n",
    "> [▶ Build a Perceptron classifier](CHEME-5800-L9a-Example-LinearModels-Classification-Perceptron-Fall-2025.ipynb). Implement the Perceptron algorithm to classify data into two categories. This exercise demonstrates how to apply linear models to classification and evaluate performance using a confusion matrix.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9521e9",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In lab `L9b`, you will implement the Perceptron algorithm to classify data into two categories based on features. This hands-on exercise develops your understanding of linear models for classification and how to evaluate performance using confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799c49c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lecture, we extended linear models from regression to binary classification tasks:\n",
    "\n",
    "> __Key takeaways:__\n",
    ">\n",
    "> 1. **Linear models for classification**: Apply activation functions $\\sigma:\\mathbb{R}\\rightarrow\\{-1,+1\\}$ to transform continuous regression outputs into discrete class labels. The Perceptron uses $\\sigma(\\star) = \\text{sign}(\\star)$ for direct classification, while logistic regression computes label probabilities.\n",
    "> 2. **Perceptron learning and convergence**: The online Perceptron learns decision boundaries through incremental updates $\\theta \\gets \\theta + y\\;\\mathbf{x}$ when misclassifications occur. For linearly separable data, convergence is guaranteed. Non-separable data requires modern optimization approaches.\n",
    "> 3. **Performance evaluation**: The $2\\times{2}$ confusion matrix shows all four prediction outcomes (TP, FP, TN, FN), enabling calculation of accuracy, precision, recall, and specificity. These metrics reveal performance trade-offs specific to your application.\n",
    "\n",
    "These concepts form the foundation for exploring probabilistic and optimization-based approaches in subsequent modules.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
