{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8148fa-3758-4fa1-9e35-2189e7fb03bd",
   "metadata": {},
   "source": [
    "# Linear Programming and Resource Allocation\n",
    "In this module, we'll introduce the core concepts of linear programming and apply these concepts to a classic class of problems: resource allocation challenges such as production planning, consumer choice, and network flow.\n",
    "\n",
    "> __Learning Objectives__\n",
    ">\n",
    "> By the end of this module, you will be able to define and demonstrate mastery of the following key concepts:\n",
    ">\n",
    "> * __Primal Linear Programming__: In the _primal problem_, we directly choose how much of an activity (e.g., production levels) to do to optimize a cost, profit, or satisfaction while respecting resource limits. Primal problems can be maximization or minimization problems.\n",
    "> * __Dual Linear Programming__: In the _dual problem_, you instead assign a _shadow price_ to each resource constraint—choosing those prices so that, if every activity were valued at its resource cost, the total value of your resources is maximized or minimized. If the primal is a maximization problem, the dual is a minimization problem, and vice versa.\n",
    "> * __Advanced: Lagrange Multipliers and Karush–Kuhn–Tucker (KKT) Conditions__: The KKT conditions for a linear program say, intuitively, that at the optimum you must satisfy all your original constraints, assign _shadow prices_ that respect the cost structure, and have any slack in a constraint paired with a zero price (and vice versa). We care about these because they give a foolproof certificate of optimality and directly drive the pivot rules in simplex and the search directions in interior-point methods.\n",
    ">\n",
    "> These methods give you tools to tackle a wide variety of real-world optimization challenges efficiently and effectively. Very useful!\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e6583",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Today, we will be using the following example(s) to illustrate key concepts:\n",
    "\n",
    "> [▶ Two Goods Resource Allocation Problem: Apples versus Oranges](CHEME-5800-L5c-FruitProblem-Primal-Example-Fall-2025.ipynb). In this example, we will explore a simple resource allocation problem involving the purchase (and consumption) of two goods: apples and oranges. We will formulate the primal linear programming problem to maximize the utility (satisfaction) while adhering to resource constraints (budget constraint). \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fe75b-fb96-461e-b087-c8dc64190d03",
   "metadata": {},
   "source": [
    "## Primal Linear Programming Problems\n",
    "Suppose you have a _linear_ objective function $O:\\mathbb{R}^{n}\\to\\mathbb{R}$ of the continuous decision variable vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ whose values are constrained by a system of $m$ linear equations. To calculate an optimal value of the decision variable vector $\\mathbf{x}$, we can formulate the problem as a _primal linear programming_ problem:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize/maximize} &\\, \\sum_{i=1}^{n} c_{i}\\;{x}_{i}\\\\\n",
    "\\text{subject to}~\\mathbf{A}\\;\\mathbf{x} &\\leq \\mathbf{b}\\quad\\mathbf{A}\\in\\mathbb{R}^{m\\times{n}}\\,\\text{and}\\,\\mathbf{b}\\in\\mathbb{R}^{m}\\\\\n",
    "~x_{i}&\\geq {0}\\qquad{i=1,2,\\dots,n}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $c_{i}\\in\\mathbb{R}$ are the coefficients of the objective function, $x_{i}\\in\\mathbb{R}$ are the decision variables, and $\\mathbf{A}$ and $\\mathbf{b}$ are the constraint matrix and right-hand side vector, respectively. The goal is to minimize or maximize the objective function while satisfying the constraints.\n",
    "\n",
    "Let's look at a few simple example problems to illustrate the primal linear programming formulation.\n",
    "\n",
    "### Consumer Choice Problems as Linear Programs\n",
    "Suppose you are a consumer with a set of products you can purchase, and you want to maximize your utility (satisfaction) from these products while staying within your budget. This is a classic example of a resource allocation task that can be formulated as a primal linear programming problem.\n",
    "\n",
    "> __Formulation__: Let there be $n$ products available for purchase. For each product $i$, let $u_{i}$ be the utility score and $c_{i}$ be the cost per unit. The consumer has a budget $I$ that they can allocate among these products. Let $x_{i}$ be the number of units of product $i$ that the consumer purchases; the total cost of the products they purchase is given by the expression $\\sum_{i=1}^{n} c_{i}\\;{x}_{i}$.\n",
    "> Finally, the consumer's utility function is defined as a linear combination of the utility scores of the products they purchase: $U\\left(x_{1},\\dots,x_{n}\\right) = \\sum_{i=1}^{n} u_{i}\\;{x}_{i}$.\n",
    "\n",
    "Putting all this together, we can formulate the consumer choice problem as the _primal_ linear program:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{maximize} &\\, \\sum_{i=1}^{n} u_{i}\\;{x}_{i} \\\\\n",
    "\\text{subject to}~\\sum_{i=1}^{n} c_{i}\\;{x}_{i}& \\leq I\\\\\n",
    "~x_{i}&\\geq{0}\\qquad{i=1,2,\\dots,n}\n",
    "\\end{align*}\n",
    "$$\n",
    "The optimal solution to this problem (if it exists) will give the consumer the optimal number of units of each product to purchase in order to maximize their utility while staying within their budget. In a similar way, we can formulate other resource allocation problems such as production planning, transportation, and network flow as primal linear programming problems.\n",
    "\n",
    "> __Example__\n",
    "> \n",
    "> [▶ Two Goods Resource Allocation Problem: Apples versus Oranges](CHEME-5800-L5c-FruitProblem-Primal-Example-Fall-2025.ipynb). In this example, we will explore a simple resource allocation problem involving the purchase (and consumption) of two goods: apples and oranges. We will formulate the primal linear programming problem to maximize the utility (satisfaction) while adhering to resource constraints (budget constraint). \n",
    "\n",
    "___\n",
    "\n",
    "### Minimum Cost Network Flow Problems as Linear Programs\n",
    "Another classic example of a resource allocation problem that can be formulated as a primal linear programming problem is the minimum cost maximum flow problem. In this problem, we have a directed (bipartite) graph with nodes representing sources, sinks, and intermediate nodes representing a matching process. The edges represent the flow of goods or resources between these nodes. Each edge has a capacity (the maximum amount of flow that can pass through it) and a cost per unit of flow.\n",
    "\n",
    "> __Formulation__: Let the directed graph be represented as $G = (\\mathcal{V}, \\mathcal{E})$, where $\\mathcal{V}$ is the set of vertices (nodes) and $\\mathcal{E}$ is the set of edges. Each edge $(i, j) \\in \\mathcal{E}$ has a capacity $c(i,j)$ and a cost (weight) $w(i,j)$ per unit of flow. Let $f_{ij}$ be the flow on edge $(i, j)$, and let $s$ be the source node and $t$ be the sink node. The goal is to maximize the flow from the source to the sink while minimizing the total cost of the flow.\n",
    "\n",
    "Putting all this together, we can formulate the minimum cost maximum flow problem as the _primal_ linear program:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} &\\, \\sum_{(i,j)\\in\\mathcal{E}} w(i,j)\\;f_{ij} \\\\\n",
    "\\text{subject to}~\\sum_{j:(i,j)\\in\\mathcal{E}} f_{ij} - \\sum_{j:(j,i)\\in\\mathcal{E}} f_{ji}&=\n",
    "\\begin{cases}\n",
    "F & \\text{if } i = s \\\\\n",
    "-F & \\text{if } i = t \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "~0 \\leq f_{ij}&\\leq c(i,j) \\quad\\forall (i,j) \\in \\mathcal{E}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $F$ is the total flow from the source to the sink. The optimal solution to this problem (if it exists) will give the flow on each edge that minimizes the total cost while satisfying the flow conservation constraints and capacity constraints.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8ad27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dual Linear Programming Problems\n",
    "Having defined primal linear programs, we now turn to their duals—alternative formulations that offer a different viewpoint on the same optimization. You can think of it as viewing the primal through a different lens.\n",
    "\n",
    "If the _primal problem_ has the form:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{maximize} &\\, \\sum_{i=1}^{n} c_{i}\\;{x}_{i}\\\\\n",
    "\\text{subject to}~\\sum_{i=1}^{n} A_{i,j}\\;{x}_{i} &\\leq b_{j}\\quad j=1,2,\\dots,m\\\\\n",
    "~x_{i}&\\geq {0}\\qquad{i=1,2,\\dots,n}\n",
    "\\end{align*}\n",
    "$$\n",
    "then the _dual problem_ has the form:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{minimize}\\quad & \\sum_{j=1}^{m} b_{j}\\,y_{j}\\\\\n",
    "\\text{subject to}\\quad & \\sum_{j=1}^{m} A_{i,j}\\,y_{j}\\;\\ge\\;c_{i}\n",
    "\\quad&&i=1,2,\\dots,n,\\\\\n",
    "&y_{j}\\;\\ge\\;0\n",
    "\\quad&&j=1,2,\\dots,m.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### What has changed?\n",
    "There are several key differences between the primal and dual linear programming problems:\n",
    "1. The objective function flips (maximum ⇒ minimum or minimum ⇒ maximum).\n",
    "2. Primal objective coefficients $c_i$ become the dual right-hand side constants.\n",
    "3. Primal right-hand side constants $b_j$ become the dual objective coefficients.\n",
    "4. The $m\\times n$ constraint matrix $A$ is transposed in the dual (so $A^\\top$ appears).\n",
    "5. The number of variables and constraints swap: the primal has $n$ variables, $m$ constraints, and the dual has $m$ variables and $n$ constraints.\n",
    "6. Each primal constraint $a_j^\\top x \\le b_j$ gives a dual variable $y_j$. Each primal variable $x_i$ gives a dual constraint $(A^\\top y)_i \\ge c_i$.\n",
    "7. Inequality directions and sign restrictions invert for the constraints: A $\\le$ constraint in the primal gives rise to a $\\ge$ constraint in the dual (and vice versa).\n",
    "8. Equality constraints in the primal become free variables in the dual, i.e., $a_j^T x = b_j$ gives rise to a dual variable $y_j$ that is free (no sign restriction), while a dual constraint $A^\\top y \\ge c$ gives rise to a primal variable $x_i$ that is free.\n",
    "\n",
    "Finally, the solutions of the primal and dual problems are related by the concept of __duality__. For a primal problem: $\\max\\{\\,c^T x : A x \\le b,\\;x\\ge0\\}$ and its corresponding dual problem: $\\min\\{\\,b^T y : A^T y \\ge c,\\;y\\ge0\\}$, the solutions are related:\n",
    "* __Weak duality__: For any primal feasible $x$ and dual feasible $y$, we have $c^T x \\le b^T y$. Thus, the primal optimum is always bounded above by the dual optimum. The difference between the two is called the _duality gap_.\n",
    "* __Strong duality__: If both primal and dual are feasible and have finite optimal values, then $\\max\\{\\,c^T x \\} = \\min\\{\\,b^T y\\}$, i.e., the _duality gap is zero_. This means that the optimal values of the primal and dual problems are equal.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95363d9",
   "metadata": {},
   "source": [
    "## Advanced Topic: Lagrange Multipliers\n",
    "Normally, when faced with a constrained optimization problem, our first thought would be to use the Lagrange multiplier method. So how does the Lagrange multiplier method work for our linear programming problem? Let's find out!\n",
    "\n",
    "We formulate the Lagrangian function by incorporating the constraints into the objective function using Lagrange multipliers. We then compute the gradient of the Lagrangian function and set it to zero to find the optimal solution. Let's apply this method to our linear programming problem. Suppose we have a linear program in the form:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} &\\, \\sum_{i=1}^{n} c_{i}\\;{x}_{i}\\\\\n",
    "\\text{subject to}~ \\mathbf{A}\\;\\mathbf{x} + \\mathbf{s} &= \\mathbf{b}\\quad\\text{where}\\,\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\,\\text{and}\\,\\mathbf{b}\\in\\mathbb{R}^{m}\\\\\n",
    "x_{i} &\\geq 0\\quad{i=1,2,\\dots,n}\\\\\n",
    "s_{j} &\\geq 0\\quad{j=1,2,\\dots,m}\n",
    "\\end{align*}$$\n",
    "where $\\mathbf{s} = (s_1, s_2, \\ldots, s_m) \\in \\mathbb{R}^{m}$ are the _slack variables_ that convert the inequalities into equalities.\n",
    "* __Why slack variables?__ Working with inequality constraints can be tricky, especially when applying the Lagrange multiplier method, which is typically designed for equality constraints. However, we can convert the inequality constraints into equality constraints by introducing _slack variables_ $s_{j}$ for each of the $m$ constraints.\n",
    "* __Definition of slack variables__: For the original inequality constraint $\\mathbf{a}^{\\top}_{i}\\cdot \\mathbf{x} \\leq b_{i}$, the slack variable $s_{i} \\geq 0$ represents the amount of \"slack\" or unused capacity in the constraint. The transformation gives us the equality constraint $\\mathbf{a}^{\\top}_{i}\\cdot \\mathbf{x} + s_{i} = b_{i}$ for each constraint $i$. This transformation is crucial because the Lagrange multiplier method requires equality constraints to define the Lagrangian function.\n",
    "\n",
    "Now, we can write the Lagrangian function $\\mathcal{L}(\\mathbf{x}, \\mathbf{s}, \\boldsymbol{\\lambda})$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathbf{x}, \\mathbf{s}, \\boldsymbol{\\lambda}) &= \\mathbf{c}^{\\top}\\mathbf{x} - \\boldsymbol{\\lambda}^{\\top}(\\mathbf{A}\\cdot\\mathbf{x} + \\mathbf{s} - \\mathbf{b})\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{m}$ are the Lagrange multipliers associated with the equality constraints. To compute the first-order optimality conditions, we take the gradient of the Lagrangian with respect to $\\mathbf{x}$, $\\mathbf{s}$, and $\\boldsymbol{\\lambda}$ and set it to zero:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{s}}\\mathcal{L}(\\mathbf{x}, \\mathbf{s}, \\boldsymbol{\\lambda}) &= -\\boldsymbol{\\lambda} = 0\\quad\\implies\\boldsymbol{\\lambda} = 0\\quad\\text{This is a problem!}\\\\\n",
    "\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, \\mathbf{s}, \\boldsymbol{\\lambda}) &= \\mathbf{c} - \\mathbf{A}^{\\top}\\boldsymbol{\\lambda} = 0\\implies\\mathbf{c} = 0\\quad\\text{This is an even bigger problem!}\\\\\n",
    "\\nabla_{\\boldsymbol{\\lambda}}\\mathcal{L}(\\mathbf{x}, \\mathbf{s}, \\boldsymbol{\\lambda}) &= \\mathbf{A}\\cdot\\mathbf{x} + \\mathbf{s} - \\mathbf{b} = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "From the first equation, the Lagrange multipliers are zero for all constraints, which then (from the second equation) requires $\\mathbf{c} = 0$, which is not generally true!\n",
    "\n",
    "__Hmmmm__. The Lagrange multiplier method doesn't work with linear programming problems. We need the _Karush-Kuhn-Tucker (KKT) conditions_ to handle the inequality constraints properly!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ba097",
   "metadata": {},
   "source": [
    "## Advanced Topic: Karush-Kuhn-Tucker (KKT) Conditions\n",
    "The Karush-Kuhn-Tucker (KKT) conditions extend the Lagrange multiplier framework to problems with inequality and non-negativity constraints. In convex settings (linear programs in particular), these conditions are necessary and sufficient for optimality.\n",
    "\n",
    "To construct the KKT conditions for a linear program, we need to understand four key concepts:\n",
    "\n",
    "* __Stationarity__: At the optimum, the competing influences of improving the objective and enforcing the constraints balance out for each decision variable, so that no infinitesimal change in any variable can increase the Lagrangian.\n",
    "* __Primal feasibility__: The candidate solution must satisfy every original model requirement (every equality condition must hold exactly, and every inequality or non-negativity restriction must be respected).\n",
    "* __Dual feasibility__: All multipliers that penalize inequality constraints must be non-negative, ensuring that they only oppose constraint violations rather than \"reward\" them.\n",
    "* __Complementary slackness__: Every inequality constraint is either exactly tight (active), in which case its multiplier may be positive, or else it is slack (not binding), in which case its multiplier is forced to zero, so that only active constraints influence the solution.\n",
    "\n",
    "Let's start by rewriting the linear program in standard form, where we introduce slack variables $s\\ge0$ for $A\\,x\\le b$, and flip the maximization problem to a minimization problem by negating the objective:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Primal LP:}\\quad\\min_{x,s}\\;c^\\top x\\quad\\text{s.t.}\\quad\n",
    "A\\,x + s = b,\\;x \\ge 0,\\;s \\ge 0.\\\\\n",
    "&\\text{Lagrange multipliers:}\\quad\n",
    "\\lambda\\in\\mathbb R^m,\\;\\mu\\in\\mathbb R^n,\\;\\nu\\in\\mathbb R^m,\n",
    "\\quad\\mu \\ge 0,\\;\\nu \\ge 0.\\\\\n",
    "&\\boxed{\\displaystyle\n",
    "\\mathcal{L}(x,s,\\lambda,\\mu,\\nu)\n",
    "= c^\\top x \n",
    "\\;-\\;\\lambda^\\top\\bigl(A\\,x + s - b\\bigr)\n",
    "\\;-\\;\\mu^\\top x\n",
    "\\;-\\;\\nu^\\top s}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where:\n",
    "* $\\lambda_j$ (unconstrained) enforces the equality constraint $\\sum_i A_{ji} x_i + s_j = b_j$,\n",
    "* $\\mu_i \\ge 0$ enforces the non-negativity constraint $x_i\\ge0$,\n",
    "* $\\nu_j \\ge 0$ enforces the non-negativity constraint $s_j\\ge0$.\n",
    "\n",
    "Now, we compute the gradient of the Lagrangian function with respect to the _primal variables_ $(x,s)$ and _dual variable_ $\\lambda$, and enforce the complementary slackness conditions for the _dual variables_ $(\\mu,\\nu)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\nabla_x\\mathcal{L}(x,s,\\lambda,\\mu,\\nu) = c - A^\\top\\lambda - \\mu = 0,\\\\\n",
    "&\\nabla_s\\mathcal{L}(x,s,\\lambda,\\mu,\\nu) = -\\lambda - \\nu = 0,\\\\\n",
    "&\\nabla_\\lambda\\mathcal{L}(x,s,\\lambda,\\mu,\\nu) = A\\,x + s - b = 0,\\\\\n",
    "&\\mu_i x_i = 0,\\quad\\forall i=1,2,\\dots,n,\\\\\n",
    "&\\nu_j s_j = 0,\\quad\\forall j=1,2,\\dots,m,\\\\\n",
    "&\\mu_i \\ge 0,\\quad\\forall i=1,2,\\dots,n,\\\\\n",
    "&\\nu_j \\ge 0,\\quad\\forall j=1,2,\\dots,m.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Great! We have the KKT conditions for the primal linear program. These conditions are necessary and sufficient for optimality in convex optimization problems, including linear programming. However, what is the actionable algorithm that we can develop from these conditions?\n",
    "\n",
    "Let's consider two classes of algorithms based on the KKT conditions: [the Revised Simplex algorithm](CHEME-5800-L5c-RevisedSimplex-Algorithm-Fall-2025.ipynb) and the [Interior Point Algorithm](CHEME-5800-L5c-InteriorPointMethod-Algorithm-Fall-2025.ipynb).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740d460",
   "metadata": {},
   "source": [
    "## Lab Exercises\n",
    "In the lab L5d, we will use [the JuMP package](https://github.com/jump-dev/JuMP.jl) in combination with [the GLPK solver](https://github.com/jump-dev/GLPK.jl) to formulate and solve linear programming problems. In particular, we'll revisit the minimum-cost maximum flow problem and solve it using linear programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40d3f9",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's all for today! What are three things you learned today?\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
