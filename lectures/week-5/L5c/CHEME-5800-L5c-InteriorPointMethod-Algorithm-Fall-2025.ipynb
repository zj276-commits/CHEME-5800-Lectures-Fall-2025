{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59e4eb8",
   "metadata": {},
   "source": [
    "# Algorithm: Interior-Point Methods\n",
    "Interior-point methods solve a linear program by traversing the _interior_ of the feasible region—avoiding the corners until the very end—using a _barrier function_ to enforce the inequalities. They trace a path defined by modified KKT conditions (with logarithmic penalties) and converge to the optimum in a small, predictable number of steps.\n",
    "\n",
    "> __The big idea__: Interior-point methods navigate the feasible region's interior using a smooth barrier that dominates at edges, taking Newton-style steps along a central path to the optimum.\n",
    "> * __Barrier function?__ An invisible wall that shoots to infinity as you approach any constraint boundary, repelling your iterates and keeping them safely in the interior while still guiding you toward the optimum. Gradually lowering its weight lets the solution creep closer to the boundary without crossing it.\n",
    "> * __Logarithmic penalties?__ An example barrier function. Logarithmic penalties are terms of the form $-\\mu\\ln(s)$ added to your objective. Because $\\ln(s)\\to-\\infty$ as $s\\to0^+$, they impose a _huge cost_ for getting too close to a constraint boundary. As you decrease the weight $\\mu$, you gradually lessen the penalty's influence, letting the solution drift closer to the true feasible region edge.\n",
    "> \n",
    "> Unlike the simplex method, which moves along the boundary and zig-zags across facets, these algorithms follow a direct, well-conditioned route through the interior, offering more predictable performance on large or ill-conditioned problems.\n",
    "\n",
    "### Algorithm\n",
    "Let's sketch out an interior-point algorithm to solve an LP.\n",
    "\n",
    "__Initialization__: Given a linear program of the form: $\\min\\left\\{c^\\top x\\mid Ax+s = b,\\;x\\ge0,\\; s\\ge0,\\;b\\ge0\\right\\}$ where $A\\in\\mathbb{R}^{m\\times{n}}$. Specify an _initial strictly feasible guess_ for $(x^{(0)}, s^{(0)}, \\lambda^{(0)}, \\nu^{(0)})$ (more on this later). Specify a tolerance $\\epsilon>0$, a maximum number of iterations $T$, an iteration counter $t\\gets{0}$, set $\\texttt{converged}\\gets\\texttt{false}$, and choose a _reduction factor_ $\\sigma\\in(0,1)$. Finally, compute $\\mu^{(0)}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu^{(0)} \\gets \\frac{x^{(0)^{\\top}}\\nu^{(0)} + s^{(0)^{\\top}}\\lambda^{(0)}}{n+m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "1. Compute the four residuals $(r_{P}, r_{D}, r^{x}_{C}, r^{s}_{C})$:\n",
    "     - _Primal residual_: $r_{P}\\gets{Ax^{(t)} +s^{(t)} - b}$. The primal residual $r_{P}$ shows how much your guess $(x,s)$ violates the constraints. Each entry of $r_P$ is the gap between your decision variables, slack variables, and the right-hand side $b$; zero indicates your solution is feasible.\n",
    "     - _Dual residual_: $r_D \\gets A^{\\top}\\lambda^{(t)}+\\nu^{(t)} - c$. The dual residual $r_{D}$ indicates how much your dual variables $(\\lambda,\\nu)$ deviate from the stationarity condition of the Lagrangian. A nonzero $r_D$ shows you need to adjust the multipliers or primal $x$ to move toward optimality.\n",
    "     - _Complementarity residual_: $r^{x}_{C}\\gets X^{(t)}\\nu^{(t)} - \\mu^{(t)}\\mathbf{1}$, where $X = \\text{diag}(x)$. The complementarity residual $r^{x}_{C}$ measures deviation from the ideal central product $x_j \\nu_j = \\mu$ for each coordinate. When $r_C^x=0$, each $x_j$ and its dual partner $\\nu_j$ satisfy the perturbed complementary-slackness condition, placing you on the central path between primal and dual feasibility.\n",
    "     - _Slack-complementarity residual_: $r^{s}_{C} \\gets S^{(t)}\\lambda^{(t)} - \\mu^{(t)}\\mathbf{1}$, where $S=\\text{diag}(s)$. The slack-complementarity residual $r^{s}_{C}$ measures how far each slack variable $s_i$ and its dual multiplier $\\lambda_i$ are from satisfying the central-path condition. When $r_C^s = 0$, each slack $s_{i}$ and multiplier $\\lambda_{i}$ satisfy $s_{i}\\lambda_{i} = \\mu$.\n",
    "\n",
    "2. Compute the Jacobian matrix $J$:\n",
    "$$\n",
    "     J =\n",
    "     \\begin{pmatrix}\n",
    "       A & I & 0 & 0\\\\\n",
    "       0 & 0 & A^\\top & I\\\\\n",
    "       \\text{diag}(\\nu^{(t)}) & 0 & 0 & \\text{diag}(x^{(t)})\\\\\n",
    "       0 & \\text{diag}(\\lambda^{(t)}) & \\text{diag}(s^{(t)}) & 0\n",
    "     \\end{pmatrix}.\n",
    "   $$\n",
    "3. Take a _Newton step_: solve for the updates $\\Delta x, \\Delta s, \\Delta \\lambda, \\Delta \\nu$:\n",
    "$$\n",
    "     J\\,\n",
    "     \\begin{pmatrix}\n",
    "       \\Delta x\\\\[3pt]\\Delta s\\\\[3pt]\\Delta\\lambda\\\\[3pt]\\Delta\\nu\n",
    "     \\end{pmatrix}\n",
    "     = -\\,\n",
    "     \\begin{pmatrix}\n",
    "       r_P\\\\r_D\\\\r_C^x\\\\r_C^s\n",
    "     \\end{pmatrix}.\n",
    "   $$\n",
    "\n",
    "4. Choose the step size $\\alpha$: Choose the largest $\\alpha\\in(0,1]$ such that:\n",
    "   $$\n",
    "     x^{(t)} + \\alpha\\,\\Delta x > 0,\\quad\n",
    "     s^{(t)} + \\alpha\\,\\Delta s > 0,\\quad\n",
    "     \\lambda^{(t)} + \\alpha\\,\\Delta\\lambda > 0,\\quad\n",
    "     \\nu^{(t)} + \\alpha\\,\\Delta\\nu > 0.\n",
    "   $$\n",
    "5. Update the system solution $(x,s,\\lambda,\\nu,\\mu)$:\n",
    " - Set $x^{(t+1)} \\gets x^{(t)} + \\alpha\\,\\Delta x$.\n",
    " - Set $s^{(t+1)} \\gets s^{(t)} + \\alpha\\,\\Delta s$.\n",
    " - Set $\\lambda^{(t+1)} \\gets \\lambda^{(t)} + \\alpha\\,\\Delta\\lambda$.\n",
    " - Set $\\nu^{(t+1)} \\gets \\nu^{(t)} + \\alpha\\,\\Delta\\nu$.\n",
    " - Set $\\mu^{(t+1)} \\gets \\sigma\\,\\mu^{(t)}$.\n",
    "\n",
    "6. Check for convergence. Update the iteration counter $t\\gets{t+1}$.\n",
    "   - If $\\mu^{(t)} \\leq \\epsilon$ (or $\\|F\\| \\leq \\epsilon$), then $(x^{(t)},s^{(t)})$ approximates the true optimum. Set $\\texttt{converged}\\gets\\texttt{true}$ and return $(x^{(t)},s^{(t)})$. Here, $F$ is the stacked residual vector.\n",
    "   - If $t>T$, we've run out of iterations. Set $\\texttt{converged}\\gets\\texttt{true}$ and return $(x^{(t)},s^{(t)})$, which is the best solution we have so far.\n",
    "\n",
    "**Relation to KKT**: At each $\\mu>0$, the algorithm solves the **perturbed** KKT system\n",
    "$\\;r_P=0,\\;r_D=0,\\;r_C^x=0,\\;r_C^s=0$,\n",
    "which approaches the **exact** (unperturbed) KKT conditions as $\\mu\\to0$.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae55973",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
