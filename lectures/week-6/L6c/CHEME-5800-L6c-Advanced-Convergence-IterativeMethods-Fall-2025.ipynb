{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00df481c",
   "metadata": {},
   "source": [
    "# Advanced: Where does spectral radius converge condition come from?\n",
    "The convergence of iterative methods depends on the properties of the system matrix $\\mathbf{A}$, and particularly on the matrix product $\\mathbf{G} = \\mathbf{M}^{-1}\\mathbf{N}$ (iteration matrix). \n",
    "\n",
    "Let's dig into the details:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{M}^{-1}\\;(\\mathbf{b} + \\mathbf{N}\\;\\mathbf{x}^{(k)})\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{M}^{-1}\\;\\mathbf{b} + \\underbrace{\\mathbf{M}^{-1}\\mathbf{N}}_{\\mathbf{G}}\\;\\mathbf{x}^{(k)}\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\underbrace{\\mathbf{M}^{-1}\\;\\mathbf{b}}_{\\mathbf{c}} + \\mathbf{G}\\;\\mathbf{x}^{(k)}\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{(k)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "At the _true_ solution $\\mathbf{x}^{\\star}$, we know that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{\\star} & = \\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{\\star}\\\\\n",
    "\\mathbf{x}^{\\star} - \\mathbf{G}\\;\\mathbf{x}^{\\star} & = \\mathbf{c}\\\\\n",
    "(\\mathbf{I} - \\mathbf{G})\\;\\mathbf{x}^{\\star} & = \\mathbf{c}\\\\\n",
    "\\mathbf{x}^{\\star} & = (\\mathbf{I} - \\mathbf{G})^{-1}\\;\\mathbf{c}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's define the error vector as $\\mathbf{e}^{(k+1)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{\\star}$. Then we can express the error in terms of the iteration matrix $\\mathbf{G}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{e}^{(k+1)} & = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{\\star}\\\\\n",
    "& = \\left(\\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{(k)}\\right) - \\left(\\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{\\star}\\right)\\\\\n",
    "& = \\mathbf{G}\\;\\mathbf{x}^{(k)} - \\mathbf{G}\\;\\mathbf{x}^{\\star}\\\\\n",
    "& = \\mathbf{G}\\;\\left(\\mathbf{x}^{(k)} - \\mathbf{x}^{\\star}\\right)\\\\\n",
    "& = \\mathbf{G}\\;\\mathbf{e}^{(k)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Then we know that the error as we march through the iterations is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{e}^{(1)} & = \\mathbf{G}\\;\\mathbf{e}^{(0)}\\\\\n",
    "\\mathbf{e}^{(2)} & = \\mathbf{G}^{2}\\;\\mathbf{e}^{(0)}\\\\\n",
    "\\vdots & \\\\\n",
    "\\mathbf{e}^{(k)} & = \\mathbf{G}^{k}\\;\\mathbf{e}^{(0)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{e}^{(0)}$ is the initial error vector, and $\\mathbf{G}^k$ is the iteration matrix raised to the power of $k$. Ultimately, we want $\\mathbf{e}^{(k)}$ to converge to zero as $k$ increases. Thus, this requires $\\mathbf{G}^{k}\\rightarrow 0$ as $k\\rightarrow \\infty$; this only occurs when $\\rho(\\mathbf{G}) < 1$ where $\\rho(\\mathbf{G})$ is the spectral radius of $\\mathbf{G}$, i.e., the largest absolute eigenvalue of $\\mathbf{G}$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5f46f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
