{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37296e72",
   "metadata": {},
   "source": [
    "# L6c: Iterative Method for Solving Linear Algebraic Equations\n",
    "In this lecture, we will be focusing on understanding a general iterative method for square solving systems of linear algebraic equations.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> Fill me in\n",
    "\n",
    "Linear algebraic equations are fundamental in various scientific and engineering applications, and iterative methods provide efficient ways to solve large systems that may be computationally expensive for direct methods. Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f946734",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Fill me in\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b8427-350c-4fc7-a973-d0f23f365290",
   "metadata": {},
   "source": [
    "## General\n",
    "Suppose we have a _square system_ of linear equations represented in matrix form as $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$,\n",
    "where the system matrix $\\mathbf{A}\\in\\mathbb{R}^{n \\times n}$, the unknown vector is $\\mathbf{x}\\in\\mathbb{R}^{n}$, and $\\mathbf{b}\\in\\mathbb{R}^{n}$ is the right-hand side vector. Our goal is to _iteratively_ find a vector $\\mathbf{x}$ that _satisfies (in some sense)_ this equation.\n",
    "\n",
    "Let's sketch the steps of a general iterative method for solving this linear system.\n",
    "\n",
    "__Initialize__: Given the system matrix $\\mathbf{A}\\in\\mathbb{R}^{n \\times n}$ and the right-hand side vector $\\mathbf{b}\\in\\mathbb{R}^{n}$, we start with an initial guess for the solution vector $\\mathbf{x}^{(0)}\\in\\mathbb{R}^{n}$. This guess can be a zero vector or any other reasonable approximation. Set $\\texttt{converged} \\gets \\texttt{false}$ and the iteration counter $k\\gets0$. Specify a convergence criterion, such as a tolerance level $\\epsilon > 0$, and a maximum number of iterations $\\texttt{max\\_iter}$.\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "1. Calculate the residual vector $\\mathbf{r}^{(k)} \\gets \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)}$.\n",
    "2. Check for convergence:\n",
    "   - If $\\|\\mathbf{r}^{(k)}\\|_{2} < \\epsilon$, __then__: set $\\texttt{converged} \\gets \\texttt{true}$. \n",
    "   - If $k \\geq \\texttt{max\\_iter}$, __then__: set $\\texttt{converged} \\gets \\texttt{true}$, and print a __warning__ that the method did not converge.\n",
    "3. Calculate an update direction. Split $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$, where $\\mathbf{M}$ is a matrix that can be easily inverted (e.g., diagonal or triangular), and $\\mathbf{N}$ is the remaining part of the matrix. The update direction can then be computed as: $\\mathbf{d}^{(k)} \\gets \\mathbf{M}^{-1}\\mathbf{r}^{(k)}$.\n",
    "4. Update the solution vector: $\\mathbf{x}^{(k+1)} \\gets \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}$.\n",
    "5. Increment the iteration counter: $k \\gets k + 1$.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058842a5-5a5f-46d2-9635-7d4702bec4d9",
   "metadata": {},
   "source": [
    "## Update direction\n",
    "The magic of iterative methods lies in the choice of the update direction. But where does this update direction come from? In the general case, we can think of the update direction as being derived from the residual vector $\\mathbf{r}^{(k)}$ and the system matrix $\\mathbf{A}$. \n",
    "\n",
    "Starting from the original system:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A}\\;\\mathbf{x} &= \\mathbf{b}\\quad\\mid\\text{substitute}\\;\\mathbf{A} = \\mathbf{M} - \\mathbf{N} \\\\\n",
    "(\\mathbf{M} - \\mathbf{N})\\;\\mathbf{x} &= \\mathbf{b} \\\\\n",
    "\\mathbf{M}\\mathbf{x}^{(k+1)} - \\mathbf{N}\\mathbf{x}^{(k)} &= \\mathbf{b}\\quad\\mid\\text{solve for}\\;\\mathbf{x}^{(k+1)}\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{M}^{-1}(\\mathbf{b} + \\mathbf{N}\\mathbf{x}^{(k)})\\quad\\mid\\text{substitute}\\;\\mathbf{M}^{-1}\\mathbf{N} = \\mathbf{I} - \\mathbf{M}^{-1}\\mathbf{A}\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{M}^{-1}\\underbrace{(\\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)})}_{\\text{residual}\\;\\mathbf{r}^{(k)}}\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{x}^{(k)} + \\underbrace{\\mathbf{M}^{-1}\\mathbf{r}^{(k)}}_{\\text{direction}\\;\\mathbf{d}^{(k)}}\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The update direction $\\mathbf{d}^{(k)}$ is derived from the residual vector $\\mathbf{r}^{(k)}$ and the system matrix $\\mathbf{A}$. The choice of $\\mathbf{M}$ and $\\mathbf{N}$ determines the specific iterative method being used, such as Jacobi, Gauss-Seidel, or Successive Over-Relaxation (SOR).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328edf5-7318-4f98-b621-4b3011330c22",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "One common question that arises when using iterative methods is: _When does the method converge?_ In other words, how do we know that the sequence of iterates $\\{\\mathbf{x}^{(k)}\\}$ will approach the true solution $\\mathbf{x}^{\\star}$ as $k$ increases? A stationary iteration $\\mathbf{x}^{(k+1)}=\\mathbf{G}\\,\\mathbf{x}^{(k)}+\\mathbf{c}$ converges __for every__ $\\mathbf{x}^{(0)}$ __if and only if__:\n",
    "$$\n",
    "\\rho(\\mathbf{G}) = \\rho\\bigl(\\mathbf{M}^{-1}\\mathbf{N}\\bigr) \\;<\\;1.\n",
    "$$\n",
    "The spectral radius $\\rho(\\mathbf{G}) = \\;\\max_i|\\lambda_i|$, where $\\lambda_i$ are the eigenvalues of the iteration matrix $\\mathbf{G}$. This means that the method will converge to the true solution regardless of the initial guess $\\mathbf{x}^{(0)}$.\n",
    "\n",
    "### Where does this come from?\n",
    "The convergence of iterative methods depends on the properties of the system matrix $\\mathbf{A}$, and particularly on the matrix product $\\mathbf{G} = \\mathbf{M}^{-1}\\mathbf{N}$ (iteration matrix). Let's dig into the details:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{M}^{-1}\\;(\\mathbf{b} + \\mathbf{N}\\;\\mathbf{x}^{(k)})\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{M}^{-1}\\;\\mathbf{b} + \\underbrace{\\mathbf{M}^{-1}\\mathbf{N}}_{\\mathbf{G}}\\;\\mathbf{x}^{(k)}\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\underbrace{\\mathbf{M}^{-1}\\;\\mathbf{b}}_{\\mathbf{c}} + \\mathbf{G}\\;\\mathbf{x}^{(k)}\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{(k)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "At the _true_ solution $\\mathbf{x}^{\\star}$, we know that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{\\star} & = \\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{\\star}\\\\\n",
    "\\mathbf{x}^{\\star} - \\mathbf{G}\\;\\mathbf{x}^{\\star} & = \\mathbf{c}\\\\\n",
    "(\\mathbf{I} - \\mathbf{G})\\;\\mathbf{x}^{\\star} & = \\mathbf{c}\\\\\n",
    "\\mathbf{x}^{\\star} & = (\\mathbf{I} - \\mathbf{G})^{-1}\\;\\mathbf{c}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's define the error vector as $\\mathbf{e}^{(k+1)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{\\star}$. Then we can express the error in terms of the iteration matrix $\\mathbf{G}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{e}^{(k+1)} & = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{\\star}\\\\\n",
    "& = \\left(\\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{(k)}\\right) - \\left(\\mathbf{c} + \\mathbf{G}\\;\\mathbf{x}^{\\star}\\right)\\\\\n",
    "& = \\mathbf{G}\\;\\mathbf{x}^{(k)} - \\mathbf{G}\\;\\mathbf{x}^{\\star}\\\\\n",
    "& = \\mathbf{G}\\;\\left(\\mathbf{x}^{(k)} - \\mathbf{x}^{\\star}\\right)\\\\\n",
    "& = \\mathbf{G}\\;\\mathbf{e}^{(k)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Then we know that the error as we march through the iterations is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{e}^{(1)} & = \\mathbf{G}\\;\\mathbf{e}^{(0)}\\\\\n",
    "\\mathbf{e}^{(2)} & = \\mathbf{G}^{2}\\;\\mathbf{e}^{(0)}\\\\\n",
    "\\vdots & \\\\\n",
    "\\mathbf{e}^{(k)} & = \\mathbf{G}^{k}\\;\\mathbf{e}^{(0)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{e}^{(0)}$ is the initial error vector, and $\\mathbf{G}^k$ is the iteration matrix raised to the power of $k$. Ultimately, we want $\\mathbf{e}^{(k)}$ to converge to zero as $k$ increases. Thus, this requires $\\mathbf{G}^{k}\\rightarrow 0$ as $k\\rightarrow \\infty$; this only occurs when $\\rho(\\mathbf{G}) < 1$.\n",
    "\n",
    "### Diagonal dominance?\n",
    "When we introduced this module, we mentioned the idea of diagonal dominance, but we developed our convergence condition in terms of the spectral radius. Are these two things related in some way? \n",
    "\n",
    "Recall that we split the matrix $\\mathbf{A}$ into its diagonal and off-diagonal components: $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$, where $\\mathbf{M}$ is the diagonal part of $\\mathbf{A}$ and $\\mathbf{N}$ is the off-diagonal part. The iteration matrix is then given by: $\\mathbf{G} = \\mathbf{M}^{-1}\\mathbf{N}$. Thus, for a matrix $\\mathbf{A}$ to be strictly diagonally dominant, we require that for each row $i$:\n",
    "$$\n",
    "    |\\mathbf{M}_{ii}| \\;>\\; \\sum_{j\\neq i} |\\mathbf{N}_{ij}|,\n",
    "$$\n",
    "This is related to the spectral radius of the iteration matrix $\\mathbf{G}$, through the induced infinity-norm:\n",
    "$$\n",
    "    \\|\\mathbf{G}\\|_\\infty \n",
    "    = \\max_i \\sum_j \\bigl| (\\mathbf{M}^{-1}\\mathbf{N})_{ij}\\bigr| \n",
    "    \\;\\le\\;\\max_i \\frac{1}{|\\mathbf{M}_{ii}|}\\sum_{j\\neq i}|\\mathbf{N}_{ij}|\n",
    "    \\;<\\;1.\n",
    "$$\n",
    "A basic property of induced norms is that for any eigenpair $(\\lambda, v)$ of $\\mathbf{G}$, for some eigenvector $v\\neq0$, then:\n",
    "$$\n",
    "|\\lambda|\\;\\|v\\|_\\infty \\;=\\;\\|\\lambda v\\|_\\infty\n",
    "\\;=\\;\\|\\mathbf{G}v\\|_\\infty\n",
    "\\;\\le\\;\\|\\mathbf{G}\\|_\\infty\\,\\|v\\|_\\infty\n",
    "\\quad\\Longrightarrow\\quad\n",
    "|\\lambda|\\le\\|\\mathbf{G}\\|_\\infty.\n",
    "$$\n",
    "Taking the maximum over all eigenvalues gives\n",
    "$\\rho(\\mathbf{G})=\\max_i|\\lambda_i|\\le\\|\\mathbf{G}\\|_\\infty.$\n",
    "\n",
    "\n",
    "### Rate of convergence?\n",
    "The rate of convergence of an iterative method is related to the spectral radius of the iteration matrix $\\mathbf{G}$. Let's consider a _worst-case_ bound. Suppose we have some induced norm $\\|\\mathbf{G}\\| = q$, then for any iteration $k$ we have:\n",
    "$$\\begin{align*}\n",
    "\\|\\mathbf{e}^{(k)}\\| & = \\lVert\\mathbf{G}^{k}\\;\\mathbf{e}^{(0)}\\rVert\\\\\n",
    "& \\le\\; \\|\\mathbf{G}^{k}\\|\\,\\|\\mathbf{e}^{(0)}\\|\\\\\n",
    "& \\le\\; q^{k}\\,\\|\\mathbf{e}^{(0)}\\|\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "To reach a desired accuracy $\\epsilon$ at iteration $k$, we can bound the error as:\n",
    "$$\n",
    "\\|\\mathbf{e}^{(k)}\\| \\leq q^{k}\\,\\|\\mathbf{e}^{(0)}\\| \\leq \\epsilon\n",
    "$$\n",
    "which implies:\n",
    "$$\n",
    "\\boxed{\n",
    "   k\\geq\\frac{\\ln(\\epsilon/\\|\\mathbf{e}^{(0)}\\|)}{|\\ln(\\lVert\\mathbf{G}\\rVert)|}\n",
    "}\\quad\\blacksquare\n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954e2b8",
   "metadata": {},
   "source": [
    "## Specific Methods\n",
    "There are several specific iterative methods that can be used to solve systems of linear equations.\n",
    "\n",
    "* __Jacobi Method__: The Jacobi method is an iterative solver that, starting from an initial guess, repeatedly refines each unknown in parallel using the residual between the right-hand side and the contributions of the other variables. It’s easy to implement and parallelize, and converges when the system matrix satisfies appropriate conditions (e.g., strict diagonal dominance). [Let's check out the algorithm.](CHEME-5800-L6c-Algorithm-JacobiMethod-Fall-2025.ipynb)\n",
    "\n",
    "* __Gauss–Seidel Method__: The Gauss–Seidel method is an iterative solver that, starting from an initial guess, refines each unknown one at a time using the most recent updates, feeding back new values immediately. This change typically yields faster convergence than Jacobi under the same matrix conditions. It’s simple to implement but inherently sequential, and converges when the system matrix is, for example, strictly diagonally dominant. [Let's check out the algorithm.](CHEME-5800-L6c-Algorithm-GaussSeidel-Fall-2025.ipynb)\n",
    "\n",
    "* __Successive Over-Relaxation (SOR) Method__: The SOR method builds on Gauss–Seidel by introducing a relaxation factor $\\omega\\in(0,2)$ that _over-relaxes_ each update, i.e., it blends the new Gauss–Seidel value with the old iterate to accelerate convergence further. With a well-chosen $\\omega$, SOR can dramatically speed up solving diagonally-dominant systems, with convergence guaranteed for $0<\\omega<2$. [Let's check out the algorithm.](CHEME-5800-L6c-Algorithm-SOR-Fall-2025.ipynb)\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
