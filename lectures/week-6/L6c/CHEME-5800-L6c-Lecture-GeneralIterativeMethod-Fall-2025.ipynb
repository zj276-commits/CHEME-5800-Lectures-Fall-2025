{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37296e72",
   "metadata": {},
   "source": [
    "# L6c: Iterative Methods for Solving Linear Algebraic Equations\n",
    "In this lecture, we will be focusing on understanding a general iterative method for solving square systems of linear algebraic equations.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lecture, you should be able to:\n",
    "> - **Outline** the general steps of an iterative method for solving linear systems, including initialization, residual calculation, convergence checking, and solution updating.\n",
    "> - **Explain** the convergence conditions for iterative methods, including the spectral radius criterion and its relation to diagonal dominance.\n",
    "> - **Describe** specific iterative methods such as Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR), including their update rules and convergence properties.\n",
    "\n",
    "Linear algebraic equations are fundamental in various scientific and engineering applications, and iterative methods provide efficient ways to solve large systems that may be computationally expensive for direct methods. Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f946734",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Today, we will be using the following example(s) to illustrate key concepts:\n",
    "\n",
    "> [▶ Fun with Iterative Methods](CHEME-5800-L6c-Example-FunWithIterativeSolvers-Fall-2025.ipynb). In this example, we will explore the implementation of various iterative methods for solving (square) systems of linear equations. We will compare the performance of these methods on randomly generated matrices and analyze their convergence behavior.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b8427-350c-4fc7-a973-d0f23f365290",
   "metadata": {},
   "source": [
    "## General\n",
    "Suppose we have a _square system_ of linear equations represented in matrix form as $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$,\n",
    "where the system matrix $\\mathbf{A}\\in\\mathbb{R}^{n \\times n}$, the unknown vector is $\\mathbf{x}\\in\\mathbb{R}^{n}$, and $\\mathbf{b}\\in\\mathbb{R}^{n}$ is the right-hand side vector. Our goal is to _iteratively_ find a vector $\\mathbf{x}$ that _satisfies (in some sense)_ this equation.\n",
    "\n",
    "Let's sketch the steps of a general iterative method for solving this linear system.\n",
    "\n",
    "__Initialize__: Given the system matrix $\\mathbf{A}\\in\\mathbb{R}^{n \\times n}$ and the right-hand side vector $\\mathbf{b}\\in\\mathbb{R}^{n}$, we start with an initial guess for the solution vector $\\mathbf{x}^{(0)}\\in\\mathbb{R}^{n}$. This guess can be a zero vector or any other reasonable approximation. Set $\\texttt{converged} \\gets \\texttt{false}$ and the iteration counter $k\\gets0$. Specify a convergence criterion, such as a tolerance level $\\epsilon > 0$, and a maximum number of iterations $\\texttt{max\\_iter}$.\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "1. Calculate the residual vector $\\mathbf{r}^{(k)} \\gets \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)}$.\n",
    "2. Check for convergence:\n",
    "   - If $\\|\\mathbf{r}^{(k)}\\|_{2} < \\epsilon$, __then__: set $\\texttt{converged} \\gets \\texttt{true}$. \n",
    "   - If $k \\geq \\texttt{max\\_iter}$, __then__: set $\\texttt{converged} \\gets \\texttt{true}$, and print a __warning__ that the method did not converge.\n",
    "3. Calculate an update direction. Split $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$, where $\\mathbf{M}$ is a matrix that can be easily inverted (e.g., diagonal or triangular), and $\\mathbf{N}$ is the remaining part of the matrix. The update direction can then be computed as: $\\mathbf{d}^{(k)} \\gets \\mathbf{M}^{-1}\\mathbf{r}^{(k)}$.\n",
    "4. Update the solution vector: $\\mathbf{x}^{(k+1)} \\gets \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}$.\n",
    "5. Increment the iteration counter: $k \\gets k + 1$.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058842a5-5a5f-46d2-9635-7d4702bec4d9",
   "metadata": {},
   "source": [
    "## Update direction\n",
    "The magic of iterative methods lies in the choice of the update direction. But where does this update direction come from? In the general case, we can think of the update direction as being derived from the residual vector $\\mathbf{r}^{(k)}$ and the system matrix $\\mathbf{A}$. \n",
    "\n",
    "Starting from the original system:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A}\\;\\mathbf{x} &= \\mathbf{b}\\quad\\mid\\text{substitute}\\;\\mathbf{A} = \\mathbf{M} - \\mathbf{N} \\\\\n",
    "(\\mathbf{M} - \\mathbf{N})\\;\\mathbf{x} &= \\mathbf{b} \\\\\n",
    "\\mathbf{M}\\mathbf{x}^{(k+1)} - \\mathbf{N}\\mathbf{x}^{(k)} &= \\mathbf{b}\\quad\\mid\\text{solve for}\\;\\mathbf{x}^{(k+1)}\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{M}^{-1}(\\mathbf{b} + \\mathbf{N}\\mathbf{x}^{(k)})\\quad\\mid\\text{substitute}\\;\\mathbf{M}^{-1}\\mathbf{N} = \\mathbf{I} - \\mathbf{M}^{-1}\\mathbf{A}\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{M}^{-1}\\underbrace{(\\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)})}_{\\text{residual}\\;\\mathbf{r}^{(k)}}\\\\\n",
    "\\mathbf{x}^{(k+1)} & = \\mathbf{x}^{(k)} + \\underbrace{\\mathbf{M}^{-1}\\mathbf{r}^{(k)}}_{\\text{direction}\\;\\mathbf{d}^{(k)}}\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The update direction $\\mathbf{d}^{(k)}$ is derived from the residual vector $\\mathbf{r}^{(k)}$ and the system matrix $\\mathbf{A}$. The choice of $\\mathbf{M}$ and $\\mathbf{N}$ determines the specific iterative method being used, such as Jacobi, Gauss-Seidel, or Successive Over-Relaxation (SOR).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328edf5-7318-4f98-b621-4b3011330c22",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "One common question that arises when using iterative methods is: _When does the method converge?_ In other words, how do we know that the sequence of iterates $\\{\\mathbf{x}^{(k)}\\}$ will approach the true solution $\\mathbf{x}^{\\star}$ as $k$ increases? A stationary iteration $\\mathbf{x}^{(k+1)}=\\mathbf{G}\\,\\mathbf{x}^{(k)}+\\mathbf{c}$ converges __for every__ $\\mathbf{x}^{(0)}$ __if and only if__:\n",
    "$$\n",
    "\\rho(\\mathbf{G}) = \\rho\\bigl(\\mathbf{M}^{-1}\\mathbf{N}\\bigr) \\;<\\;1.\n",
    "$$\n",
    "The spectral radius $\\rho(\\mathbf{G}) = \\;\\max_i|\\lambda_i|$, where $\\lambda_i$ are the eigenvalues of the iteration matrix $\\mathbf{G}$. This means that the method will converge to the true solution regardless of the initial guess $\\mathbf{x}^{(0)}$.\n",
    "\n",
    "> __Additional Notes:__ The derivation of the spectral radius convergence condition is provided in the [Advanced: Where does spectral radius convergence condition come from?](CHEME-5800-L6c-Advanced-Convergence-IterativeMethods-Fall-2025.ipynb) notebook. Check it out if you are interested in the mathematical details behind this condition.\n",
    "\n",
    "### Diagonal dominance?\n",
    "When we introduced this lecture, we mentioned the idea of diagonal dominance, but we developed our convergence condition in terms of the spectral radius. Are these two things related in some way? \n",
    "\n",
    "Recall that we split the matrix $\\mathbf{A}$ into its diagonal and off-diagonal components: $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$, where $\\mathbf{M}$ is the diagonal part of $\\mathbf{A}$ and $\\mathbf{N}$ is the off-diagonal part. The iteration matrix is then given by: $\\mathbf{G} = \\mathbf{M}^{-1}\\mathbf{N}$. Thus, for a matrix $\\mathbf{A}$ to be strictly diagonally dominant, we require that for each row $i$:\n",
    "$$\n",
    "    |\\mathbf{M}_{ii}| \\;>\\; \\sum_{j\\neq i} |\\mathbf{N}_{ij}|,\n",
    "$$\n",
    "This is related to the spectral radius of the iteration matrix $\\mathbf{G}$, through the induced infinity-norm:\n",
    "$$\n",
    "    \\|\\mathbf{G}\\|_\\infty \n",
    "    = \\max_i \\sum_j \\bigl| (\\mathbf{M}^{-1}\\mathbf{N})_{ij}\\bigr| \n",
    "    \\;\\le\\;\\max_i \\frac{1}{|\\mathbf{M}_{ii}|}\\sum_{j\\neq i}|\\mathbf{N}_{ij}|\n",
    "    \\;<\\;1.\n",
    "$$\n",
    "A basic property of induced norms is that for any eigenpair $(\\lambda, v)$ of $\\mathbf{G}$, for some eigenvector $v\\neq0$, then:\n",
    "$$\n",
    "|\\lambda|\\;\\|v\\|_\\infty \\;=\\;\\|\\lambda v\\|_\\infty\n",
    "\\;=\\;\\|\\mathbf{G}v\\|_\\infty\n",
    "\\;\\le\\;\\|\\mathbf{G}\\|_\\infty\\,\\|v\\|_\\infty\n",
    "\\quad\\Longrightarrow\\quad\n",
    "|\\lambda|\\le\\|\\mathbf{G}\\|_\\infty.\n",
    "$$\n",
    "Taking the maximum over all eigenvalues gives\n",
    "$\\rho(\\mathbf{G})=\\max_i|\\lambda_i|\\le\\|\\mathbf{G}\\|_\\infty.$\n",
    "\n",
    "\n",
    "### Rate of convergence?\n",
    "The rate of convergence of an iterative method is related to the spectral radius of the iteration matrix $\\mathbf{G}$. Let's consider a _worst-case_ bound. Suppose we have some (induced) norm $\\|\\mathbf{G}\\| = q$, then for any iteration $k$ we have:\n",
    "$$\\begin{align*}\n",
    "\\|\\mathbf{e}^{(k)}\\| & = \\lVert\\mathbf{G}^{k}\\;\\mathbf{e}^{(0)}\\rVert\\\\\n",
    "& \\le\\; \\|\\mathbf{G}^{k}\\|\\,\\|\\mathbf{e}^{(0)}\\|\\\\\n",
    "& \\le\\; q^{k}\\,\\|\\mathbf{e}^{(0)}\\|\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "To reach a desired accuracy $\\epsilon$ at iteration $k$, we can bound the error as:\n",
    "$$\n",
    "\\|\\mathbf{e}^{(k)}\\| \\leq q^{k}\\,\\|\\mathbf{e}^{(0)}\\| \\leq \\epsilon\n",
    "$$\n",
    "which implies:\n",
    "$$\n",
    "\\boxed{\n",
    "   k\\geq\\frac{\\ln(\\epsilon/\\|\\mathbf{e}^{(0)}\\|)}{|\\ln(\\lVert\\mathbf{G}\\rVert)|}\n",
    "}\\quad\\blacksquare\n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954e2b8",
   "metadata": {},
   "source": [
    "## Specific Methods\n",
    "There are several specific iterative methods that can be used to solve systems of linear equations.\n",
    "\n",
    "* __Jacobi Method__: The Jacobi method is an iterative solver that, starting from an initial guess, repeatedly refines each unknown in parallel using the residual between the right-hand side and the contributions of the other variables. It’s easy to implement and parallelize, and converges when the system matrix satisfies appropriate conditions (e.g., strict diagonal dominance). [Let's check out the algorithm.](CHEME-5800-L6c-Algorithm-JacobiMethod-Fall-2025.ipynb)\n",
    "\n",
    "* __Gauss–Seidel Method__: The Gauss–Seidel method is an iterative solver that, starting from an initial guess, refines each unknown one at a time using the most recent updates, feeding back new values immediately. This change typically yields faster convergence than Jacobi under the same matrix conditions. It’s simple to implement but inherently sequential, and converges when the system matrix is, for example, strictly diagonally dominant. [Let's check out the algorithm.](CHEME-5800-L6c-Algorithm-GaussSeidel-Fall-2025.ipynb)\n",
    "\n",
    "* __Successive Over-Relaxation (SOR) Method__: The SOR method builds on Gauss–Seidel by introducing a relaxation factor $\\omega\\in(0,2)$ that _over-relaxes_ each update, i.e., it blends the new Gauss–Seidel value with the old iterate to accelerate convergence further. With a well-chosen $\\omega$, SOR can dramatically speed up solving diagonally-dominant systems, with convergence guaranteed for $0<\\omega<2$. [Let's check out the algorithm.](CHEME-5800-L6c-Algorithm-SOR-Fall-2025.ipynb)\n",
    "\n",
    "Let's look at an example of each of these methods in action!\n",
    "\n",
    "> __Example__\n",
    ">\n",
    "> [▶ Fun with Iterative Methods](CHEME-5800-L6c-Example-FunWithIterativeSolvers-Fall-2025.ipynb). In this example, we will explore the implementation of various iterative methods for solving (square) systems of linear equations. We will compare the performance of these methods on randomly generated matrices and analyze their convergence behavior.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4554a94",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In the L6b we will be doing a deeper dive into the implementation of these iterative methods, and we will compare their performance on a classical test problem (a system of linear equations arising from the discretization of the 2D Poisson partial differential equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f02601",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lecture, we explored iterative methods for solving linear systems, focusing on the general algorithm, convergence conditions, and specific implementations.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> - **General Algorithm**: Iterative methods start with an initial guess and update the solution using residuals, checking for convergence based on tolerance or iteration limits.\n",
    "> - **Convergence Criteria**: Methods converge when the spectral radius of the iteration matrix is less than 1, which is related to diagonal dominance of the system matrix.\n",
    "> - **Specific Methods**: Jacobi, Gauss-Seidel, and SOR differ in how they compute updates, with SOR using a relaxation factor for potentially faster convergence.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
