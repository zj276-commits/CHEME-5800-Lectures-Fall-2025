{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbdeed61-acbf-4b26-a5d3-8a78609e5f37",
   "metadata": {},
   "source": [
    "# Example: Fun with Iterative Linear Algebraic Equation (LAE) Solvers\n",
    "In this example, let's explore the properties and performance of three iterative linear algebraic solvers: the Jacobi method, the Gauss-Seidel method, and the Successive Over-Relaxation (SOR) method. \n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> By the end of this activity, you should be able to:\n",
    "> * __Random System Generation__: Generate random diagonally dominant system matrices $\\mathbf{A}$ and right-hand-side vectors $\\mathbf{b}$ of a specified dimension. We'll use these test matrices for benchmarking studies later in the activity.\n",
    "> * __Solve a random square system__: Solve the LAEs using the Jacobi, Gauss-Seidel and SOR methods. In this task, we'll solve our system of random linear algebraic equations using the [Jacobi](https://en.wikipedia.org/wiki/Jacobi_method), [Gauss-Seidel](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method), and [SOR](https://en.wikipedia.org/wiki/Successive_over-relaxation) methods.\n",
    "> * __Benchmark__: In this task, we'll compare the runtime performance of the different iterative approaches against the built-in method implemented by [the LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/) included with Julia [using the `BenchmarkTools.jl` package.](https://github.com/JuliaCI/BenchmarkTools.jl)\n",
    "\n",
    "This should be fun, so let's go!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f00ccd",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> __Include:__ The [include command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's setup your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac23e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c30e4",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl), check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types and data used in this material. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1658a",
   "metadata": {},
   "source": [
    "## Task 1: Build random diagonally dominant matrices\n",
    "In this task, we'll generate a random system matrix $\\mathbf{A}$ that is diagonally dominant and a random right-hand side vector $\\mathbf{b}$. \n",
    "\n",
    "\n",
    "> __Diagonal dominance__ is a matrix property where the absolute value of the diagonal element of each row is greater than the sum of the absolute values of the other elements in that row.  Diagonal dominance is a sufficient (but not necessary) condition for convergence of iterative methods, however, this condition says nothing about the rate of convergence.\n",
    "\n",
    "A diagonally dominant system matrix $\\mathbf{A}$ has the feature:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sum_{j=1,i}^{n}\\lvert{a_{ij}}\\rvert<\\lvert{a_{ii}}\\rvert\\qquad\\forall{i}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's start by specifying how many rows we have in the _square_ system matrix $\\mathbf{A}$ in the `number_of_rows::Int64` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea1d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = 100; # increase the number of rows for larger problem size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64f893",
   "metadata": {},
   "source": [
    "Next, we generate a $n\\times{n}$ random system matrix $\\mathbf{A}$ and a $n\\times{1}$ random vector $\\mathbf{b}$, [using the `randn(...)` method](https://docs.julialang.org/en/v1/stdlib/Random/#Base.randn). We add some extra to the diagonal elements of the test system matrix $\\mathbf{A}$ to ensure diagonal dominance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4cc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A,b = let\n",
    "\n",
    "    # initialize -\n",
    "    ϵ = 10.0; # extra stuff that we add to diagonal elements\n",
    "    A = (1/ϵ)*rand(number_of_rows, number_of_rows) .+ ϵ*diagm(rand(number_of_rows)); # Interesting!\n",
    "    b = ϵ*rand(number_of_rows);\n",
    "\n",
    "    A,b\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e61c1",
   "metadata": {},
   "source": [
    "What does $\\mathbf{A}$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bb447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×100 Matrix{Float64}:\n",
       " 5.80266     0.0854931   0.0333791   …  0.0525582   0.096487    0.0368234\n",
       " 0.017033    8.42437     0.0487222      0.064586    0.0598619   0.0980186\n",
       " 0.0623465   0.0919301   9.39847        0.078199    0.00483644  0.0340437\n",
       " 0.0247749   0.0385506   0.0795655      0.0219072   0.0756997   0.0494401\n",
       " 0.0474572   0.0782414   0.00584511     0.0923643   0.0805831   0.0266333\n",
       " 0.0259679   0.0717777   0.0240308   …  0.0318752   0.0237737   0.0559262\n",
       " 0.0863729   0.00915359  0.0420482      0.083373    0.00947534  0.0331111\n",
       " 0.0804549   0.0467689   0.0397823      0.0143953   0.0928532   0.0627972\n",
       " 0.0763584   0.0415765   0.0170317      0.0614015   0.0240338   0.0162323\n",
       " 0.00983324  0.0810885   0.0456119      0.0598723   0.0475709   0.0428205\n",
       " ⋮                                   ⋱                          \n",
       " 0.0958606   0.0709765   0.0102805      0.0711612   0.0853803   0.0341746\n",
       " 0.0684312   0.0294139   0.0673111      0.0650902   0.0240292   0.0407567\n",
       " 0.0726067   0.0074649   0.0761336      0.0937769   0.0180223   0.0367404\n",
       " 0.0539394   0.0165545   0.0304308      0.00747982  0.0877269   0.0782415\n",
       " 0.0606734   0.0779217   0.0230899   …  0.0286973   0.00541666  0.0857419\n",
       " 0.0449572   0.0661416   0.0926348      0.0265132   0.0446103   0.0641061\n",
       " 0.0842658   0.026403    0.00394544     7.26591     0.094397    0.0453434\n",
       " 0.0847863   0.0865189   0.0420754      0.020375    0.864748    0.00193273\n",
       " 0.0640906   0.010726    0.0757343      0.00324082  0.0605112   5.75677"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0c43f",
   "metadata": {},
   "source": [
    "### Check: Is the system matrix $\\mathbf{A}$ strictly diagonally dominant?\n",
    "Before we continue, let's verify the randomly generated system matrix $\\mathbf{A}$ is actually diagonally dominant.\n",
    "> __Test:__ We'll compute the sum of the absolute values of each row (excluding the diagonal element), and compare it to the absolute value of the diagonal element in that row. If the sum of the absolute values of the non-diagonal elements is less than the absolute value of the diagonal element, then the matrix is diagonally dominant.\n",
    "\n",
    "Is the matrix $\\mathbf{A}$ strictly diagonally dominant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77fd43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddcondition = let\n",
    "    \n",
    "    # initialize -\n",
    "    ddcondition = Array{Bool,1}(undef, number_of_rows);\n",
    "\n",
    "    # let's check each row\n",
    "    for i ∈ 1:number_of_rows\n",
    "        aii = abs(A[i,i]);\n",
    "        σ = 0.0;\n",
    "        for j ∈ 1:number_of_rows\n",
    "            if (i ≠ j)\n",
    "                σ += abs(A[i,j]);\n",
    "            end\n",
    "        end\n",
    "        ddcondition[i] = (aii > σ) ? true : false; # ternary operator, nice!\n",
    "    end\n",
    "\n",
    "    ddcondition\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8aff5",
   "metadata": {},
   "source": [
    "### Understanding the Diagonal Dominance Check\n",
    "\n",
    "The `ddcondition` array contains boolean values indicating whether each row of our system matrix $\\mathbf{A}$ satisfies the diagonal dominance condition. Each element `ddcondition[i]` is `true` if the absolute value of the diagonal element in row `i` is greater than the sum of the absolute values of all other elements in that row, and `false` otherwise.\n",
    "\n",
    "> __Test:__ The assertion `@assert any(ddcondition)` checks that at least one row (and hopefully all rows) satisfies the diagonal dominance condition. If the condition fails, a warning is issued indicating that the matrix may not be suitable for iterative methods, as convergence is not guaranteed.\n",
    "\n",
    "For diagonally dominant matrices, iterative methods like Jacobi, Gauss-Seidel, and SOR are guaranteed to converge, making them reliable choices for solving linear systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40095d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "try\n",
    "    @assert any(ddcondition)\n",
    "catch\n",
    "    @warn \"Diagonal dominance condition not satisfied\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea18485",
   "metadata": {},
   "source": [
    "__No warnings?__ Great, let's move on to solving the system of linear algebraic equations (LAEs) using one of our iterative methods.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc5115",
   "metadata": {},
   "source": [
    "## Task 2: Let's test our iterative solvers\n",
    "In this task, let's test the correctness of our iterative linear solver implementations. \n",
    "\n",
    "> __Idea:__ We'll solve the system of linear algebraic equations $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ using the Jacobi, Gauss-Seidel, and SOR methods, and compare the results to the solution obtained using [Julia's built-in backslash operator (`\\`)](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#Base.:\\\\-Tuple{AbstractMatrix,%20AbstractVecOrMat}), which uses a polyalgorithm approach, where the choice of a specific method depends on the problem structure and size.\n",
    "\n",
    "First, let's use the backslash operator to obtain the Julia solution (which we will use as a reference). We'll store the reference solution the `julia_solution::Array{Float64,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f46225",
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_solution = A\\b; # Julia solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cca50",
   "metadata": {},
   "source": [
    "Next, let's set up a code block to compute the solutions using the Jacobi, Gauss-Seidel, and SOR methods. We'll compare the results to the Julia solution we got earlier.\n",
    "\n",
    "\n",
    "> __Explanation:__ In the following code block, we'll solve our linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ using one of our iterative methods. \n",
    "> \n",
    "> * The code sets up the algorithm parameters including maximum iterations, convergence tolerance, and initial guess, then calls our custom solver implementation. \n",
    "> * The solver returns a dictionary (`our_solution_archive::Dict{Int64, Vector{Float64}}`) that stores the solution vector (value) at each iteration (key), allowing us to track the convergence behavior and retrieve the final converged solution. \n",
    "> * You can switch between different iterative methods (Jacobi, Gauss-Seidel, or SOR) by changing the `algorithm` parameter.\n",
    "\n",
    "So what do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce4ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_solution_archive = let\n",
    "\n",
    "    # initialize -\n",
    "    maximum_number_of_iterations = 1000;\n",
    "    tolerance = 1e-12;\n",
    "    algorithm = GaussSeidelMethod(); # change this to GaussSeidelMethod() or SuccessiveOverRelaxationMethod() to test other algorithms\n",
    "    ω = 1.0; # relaxation factor, only used for SOR\n",
    "    xₒ = 0.1*ones(number_of_rows); # initial solution guess xₒ\n",
    "\n",
    "    # call the solve method with the appropriate parameters -\n",
    "    x = VLDataScienceMachineLearningPackage.solve(A, b, xₒ; algorithm = algorithm, \n",
    "        maxiterations = maximum_number_of_iterations, \n",
    "        ϵ = tolerance, \n",
    "        ω = ω # only used for SOR\n",
    "    );\n",
    "\n",
    "    x; # return the solution\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fada0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Vector{Float64}} with 35 entries:\n",
       "  5  => [0.711777, 0.55777, 0.587827, 0.91378, -0.468157, -0.400255, 3.46543, 0…\n",
       "  16 => [0.705763, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  20 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  12 => [0.705768, 0.551011, 0.582947, 0.909263, -0.472278, -0.401397, 3.4455, …\n",
       "  24 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  28 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  8  => [0.705436, 0.550633, 0.582674, 0.90903, -0.472495, -0.401454, 3.44442, …\n",
       "  17 => [0.705763, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  30 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  1  => [1.24058, 0.932213, 0.79183, 1.4298, 0.291202, -0.00711807, 5.62651, 1.…\n",
       "  19 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  0  => [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1  …  0.1, 0.1, 0.1, 0.…\n",
       "  22 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  6  => [0.707444, 0.553908, 0.584447, 0.913998, -0.470406, -0.40053, 3.45463, …\n",
       "  23 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  11 => [0.705781, 0.551021, 0.582957, 0.909258, -0.472273, -0.401397, 3.44553,…\n",
       "  32 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  9  => [0.705662, 0.550831, 0.582853, 0.90898, -0.472392, -0.401449, 3.44494, …\n",
       "  31 => [0.705762, 0.551001, 0.582942, 0.909248, -0.472284, -0.4014, 3.44547, 0…\n",
       "  ⋮  => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "our_solution_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc74d8",
   "metadata": {},
   "source": [
    "### Check: Solution consistency\n",
    "Before we proceed, let's check the difference between the solution produced by Julia and the solution we computed using our iterative solution techniques. \n",
    "\n",
    "Let's compute the magnitude of the residual vector between our solution and the Julia solution using a [norm function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.norm) exported by [the `LinearAlgebra.jl` package included with the standard library](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#man-linalg).\n",
    "\n",
    "> __Vector norm__\n",
    ">\n",
    "> A function $|\\cdot|:\\mathbb{K}^{n}\\to\\;[0,\\infty)$ ($\\mathbb{K}=\\mathbb{R}$ or $\\mathbb{C}$) is a **norm** if, for all $u,v\\in\\mathbb{K}^n$ and $\\alpha\\in\\mathbb{K}$,\n",
    ">\n",
    "> 1. **Definiteness:** $|v|\\ge 0$ and $|v|=0 \\iff v=0$.\n",
    "> 2. **Homogeneity:** $|\\alpha v|=|\\alpha|\\times|v|$.\n",
    "> 3. **Triangle inequality:** $|u+v|\\le |u|+|v|$.\n",
    "\n",
    "Here, we will use a particular norm, the p-norm:\n",
    "\n",
    "> **Definition ($p$-norm / $\\ell_p$ norm).**\n",
    "> \n",
    "> Let $\\mathbb{K}\\in{\\mathbb{R},\\mathbb{C}}$ and $x=(x_1,\\ldots,x_n)\\in\\mathbb{K}^n$. Then, for $1\\le p<\\infty$ the p-norm is given by:\n",
    "> $$\n",
    "> \\|x\\|_{p}=\\Big(\\sum_{i=1}^n |x_i|^{\\,p}\\Big)^{1/p}.\n",
    "> $$\n",
    "> and for $p=\\infty$:\n",
    "> $$\n",
    "> \\|x\\|_{\\infty}=\\max_{1\\le i\\le n}|x_i|.\n",
    "> $$\n",
    "\n",
    "Let's keep it simple and use the $p = 2$ norm for our residual calculation (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa46e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    atol = 1e-8; # absolute tolerance\n",
    "    i = keys(our_solution_archive) |> collect |> sort |> last; # get the last key (the last iteration)\n",
    "    last_solution = our_solution_archive[i]; # get the last solution (converged solution)\n",
    "\n",
    "    r = last_solution - julia_solution; \n",
    "    norm_r = norm(r) # 2-norm of the residual\n",
    "\n",
    "    # check: our solution vs Julia solution, should be approximately equal\n",
    "    @assert isapprox(norm_r, 0, atol=atol)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25937ccc",
   "metadata": {},
   "source": [
    "If we don't have any error, then our iterative solver is working correctly! But how well does it perform compared to Julia's built-in solver? Let's check that question out next.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc6da3",
   "metadata": {},
   "source": [
    "## Task 3: Let's compare the performance of our solvers to the built-in Julia solvers\n",
    "In this task, we'll benchmark our iterative solvers against Julia's built-in solvers to see how they perform on the same problem.\n",
    "\n",
    ">__Expectation:__ We expect Julia's built-in solvers to be highly optimized and potentially __much__ faster than our iterative solvers, especially for larger problem sizes. However, our solvers may still perform competitively for smaller problems or specific cases.\n",
    "\n",
    "Let's start with Julia's built-in solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74f69c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m48.000 μs\u001b[22m\u001b[39m … \u001b[35m 2.693 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 96.83%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m48.667 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m50.851 μs\u001b[22m\u001b[39m ± \u001b[32m44.099 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.56% ±  3.55%\n",
       "\n",
       "  \u001b[39m▄\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▆\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  48 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      64.4 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m81.89 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m7\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark $A\\$b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f7b1f",
   "metadata": {},
   "source": [
    "Now, let's benchmark our iterative solvers against Julia's built-in solvers to see how they perform on the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5398f016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m135.084 μs\u001b[22m\u001b[39m … \u001b[35m 2.474 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 29.09%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m139.583 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m158.992 μs\u001b[22m\u001b[39m ± \u001b[32m93.334 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m7.42% ± 11.79%\n",
       "\n",
       "  \u001b[39m█\u001b[34m▄\u001b[39m\u001b[39m▂\u001b[32m▃\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m \u001b[39m█\n",
       "  135 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       558 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m598.06 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m281\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    maximum_number_of_iterations = 10000;\n",
    "    tolerance = 1e-8;\n",
    "    algorithm = GaussSeidelMethod(); # change this to GaussSeidelMethod() or SuccessiveOverRelaxationMethod() to test other algorithms\n",
    "    ω = 0.6; # relaxation factor, only used for SOR\n",
    "    xₒ = 0.1*ones(number_of_rows); # initial solution guess xₒ\n",
    "\n",
    "     # call the solve method with the appropriate parameters -\n",
    "    @benchmark VLDataScienceMachineLearningPackage.solve($A, $b, $xₒ; algorithm = $algorithm, \n",
    "        maxiterations = $maximum_number_of_iterations, \n",
    "        ϵ = $tolerance, \n",
    "        ω = $ω # only used for SOR\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979bb73",
   "metadata": {},
   "source": [
    "### What did we see?\n",
    "The Julia implementation is efficient, even for larger problem sizes. The iterative solvers we implemented perform comparably (similar order of magnitude median runtime) to the built-in solvers, but there are some notable differences in memory usage and convergence behavior.\n",
    "\n",
    "> __Differences:__ Our iterative solvers consistently use significantly more memory than the built-in solvers, particularly for larger problem sizes. Furthermore, our solvers have more allocations. This is likely due to the way we store and manipulate intermediate results (e.g., for informational purposes we keep all intermediate solutions). Occasionally, our solvers may diverge or exhibit slower convergence compared to the built-in methods.\n",
    "\n",
    "Another data point in the ongoing story that is __buy versus build__. Unless you need some specialized behavior, it's almost always better to use established libraries and frameworks rather than building your own solutions from scratch!\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
