{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86ea67d",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "A Markov Decision Process extends the basic Markov chain framework by introducing actions and rewards, enabling agents to make optimal decisions in stochastic environments. While standard Markov chains evolve passively according to fixed transition probabilities, MDPs allow an agent to choose actions that influence state transitions and generate rewards, forming the foundation for sequential decision-making under uncertainty.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this module, you will be able to:\n",
    ">\n",
    "> * __MDP Components:__ Understand how states, actions, rewards, transition probabilities, and discount factors work together to define a sequential decision problem. Learn how policies map states to actions and how the discount factor balances immediate versus long-term rewards.\n",
    "> * __Value Iteration Algorithm:__ Apply the Bellman backup operation to compute optimal value functions and extract optimal policies. Understand how iterative updates converge to the maximum expected cumulative discounted reward from each state.\n",
    "> * __Random Rollout Sampling:__ Estimate state values through Monte Carlo simulation by generating random trajectories and averaging cumulative discounted rewards. Recognize how model-free approaches enable learning from direct experience without requiring transition probabilities.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1804131",
   "metadata": {},
   "source": [
    "## MDP Components\n",
    "Formally, an MDP is defined by the tuple $\\left(\\mathcal{S}, \\mathcal{A}, R\\left(s, a\\right), T\\left(s^{\\prime}\\,|\\,s,a\\right), \\gamma\\right)$:\n",
    "\n",
    "* **State space** $\\mathcal{S}$: The set of all possible states $s$ the system can occupy.\n",
    "* **Action space** $\\mathcal{A}$: The set of all possible actions $a$ available to the agent. For a given state $s$, the available actions are denoted as $\\mathcal{A}_{s} \\subseteq \\mathcal{A}$.\n",
    "* **Reward function** $R\\left(s, a\\right)$: The immediate reward received when taking action $a$ in state $s$.\n",
    "* **Transition model** $T\\left(s^{\\prime}\\,|\\,s,a\\right) = P(s_{t+1} = s^{\\prime}\\,|\\,s_{t}=s,a_{t} = a)$: The probability that action $a$ in state $s$ at time $t$ results in state $s^{\\prime}$ at time $t+1$.\n",
    "* **Discount factor** $\\gamma \\in [0,1]$: A parameter that weights future rewards relative to immediate rewards, with $\\gamma = 0$ prioritizing immediate rewards and $\\gamma \\to 1$ valuing long-term returns.\n",
    "\n",
    "The agent's goal is to find a policy $\\pi: \\mathcal{S} \\to \\mathcal{A}$ that maps states to actions ($\\pi(s) = a$) while maximizing the expected cumulative discounted reward over time. \n",
    "\n",
    "Two common approaches for solving MDPs are __Value Iteration__ and __Random Rollout__ algorithms.\n",
    "\n",
    "### Example: Grid World Navigation\n",
    "\n",
    "Consider a robot navigating a $3\\times 3$ grid world where the goal is to reach a target location while avoiding obstacles and minimizing travel time. This concrete example illustrates how the abstract MDP components work together:\n",
    "\n",
    "* **State space** $\\mathcal{S}$: The set of 9 grid positions, represented as $(i,j)$ coordinates where $i,j \\in \\{1,2,3\\}$.\n",
    "* **Action space** $\\mathcal{A}$: The set $\\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$. At boundary positions, actions that would move the agent outside the grid are unavailable, so $\\mathcal{A}_s \\subsetneq \\mathcal{A}$ for edge and corner states.\n",
    "* **Reward function** $R(s,a)$: The agent receives $-1$ for each step (encouraging shorter paths), $+10$ for reaching the goal at position $(3,3)$, and $-5$ for attempting to move into an obstacle at position $(2,2)$.\n",
    "* **Transition model** $T(s'|s,a)$: Actions are stochastic due to slippery terrain. The intended direction succeeds with probability $0.8$, while the two perpendicular directions each occur with probability $0.1$. For example, choosing \"up\" results in moving up with probability $0.8$, and moving left or right with probability $0.1$ each.\n",
    "* **Discount factor** $\\gamma = 0.9$: This values the $+10$ goal reward over immediate $-1$ step costs, encouraging the agent to pursue the goal despite accumulated penalties.\n",
    "\n",
    "In this setting, an optimal policy $\\pi^{*}$ would map each grid position to the action that maximizes expected cumulative discounted reward, accounting for both the stochastic transitions and the spatial layout of rewards and obstacles.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585f3db",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "Value iteration is a dynamic programming algorithm that computes the optimal value function $U^{*}(s)$ by iteratively applying the Bellman backup operation:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "U_{k+1}(s) = \\max_{a\\in\\mathcal{A}_s}\\left(R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U}_{k}(s^{\\prime})\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "As $k \\to \\infty$, the value function converges such that $U_k(s) \\to U^{*}(s)$. The optimal value function represents the maximum expected cumulative discounted reward achievable from each state under the best possible policy.\n",
    "\n",
    "#### Algorithm\n",
    "The value iteration algorithm computes the optimal value function $U^{*}(s)$ and policy $\\pi^{*}(s)$.\n",
    "\n",
    "__Initialize__: Given an MDP with state space $\\mathcal{S}$, action space $\\mathcal{A}$, reward function $R(s,a)$, transition model $T\\left(s^{\\prime}\\,|\\,s,a\\right)$, discount factor $\\gamma$, tolerance parameter $\\epsilon$, and maximum number of iterations $T$, initialize the iteration counter $k\\gets 0$, the initial value function $U_{0}(s) \\gets 0$ for all $s \\in \\mathcal{S}$, and $\\texttt{converged}\\gets\\texttt{false}$.\n",
    "\n",
    "While $\\texttt{converged}$ is $\\texttt{false}$ __do__:\n",
    "1. For each state $s \\in \\mathcal{S}$, compute the updated value:\n",
    "   $$U_{k+1}(s) \\gets \\max_{a\\in\\mathcal{A}_s}\\left(R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U}_{k}(s^{\\prime})\\right)$$\n",
    "2. Check for convergence:\n",
    "    - If $\\max_{s\\in\\mathcal{S}} \\left|U_{k+1}(s) - U_{k}(s)\\right| \\leq \\epsilon$, then set $\\texttt{converged}\\gets\\texttt{true}$ and $U^{*}\\gets{U}_{k+1}$.\n",
    "    - If $\\max_{s\\in\\mathcal{S}} \\left|U_{k+1}(s) - U_{k}(s)\\right| > \\epsilon$, update $k\\gets{k+1}$ and $U_{k}\\gets{U}_{k+1}$.\n",
    "3. Update the $\\texttt{converged}$ flag:\n",
    "    - If $k\\geq{T}$, then set $\\texttt{converged}\\gets\\texttt{true}$ and $U^{*}\\gets{U}_{k+1}$.\n",
    "\n",
    "__Extract Policy__: For each state $s \\in \\mathcal{S}$, compute:\n",
    "$$\\pi^{*}(s) \\gets \\arg\\max_{a\\in\\mathcal{A}_s}\\left(R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U^{*}}(s^{\\prime})\\right)$$\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "Value iteration requires $O(|\\mathcal{S}|^2 \\cdot |\\mathcal{A}|)$ operations per iteration. For each of the $|\\mathcal{S}|$ states, we must evaluate $|\\mathcal{A}|$ actions, and each action evaluation requires summing over $|\\mathcal{S}|$ possible next states. The number of iterations until convergence depends on the discount factor $\\gamma$ and the tolerance $\\epsilon$, with convergence rate proportional to $\\gamma$.\n",
    "\n",
    "This complexity makes value iteration tractable for problems with thousands of states but challenging for continuous or very large discrete state spaces. In such cases, function approximation or sampling-based methods become necessary.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96ee9e",
   "metadata": {},
   "source": [
    "## Random Rollout Algorithm\n",
    "\n",
    "The random rollout algorithm estimates state values by simulating random trajectories from a starting state and computing the cumulative discounted reward obtained along each path. Unlike value iteration, which requires knowledge of the transition model $T(s^{\\prime}\\,|\\,s,a)$ and computes values for all states simultaneously, random rollout is a model-free sampling approach that explores the state space through direct interaction with the environment.\n",
    "\n",
    "The algorithm generates trajectories by selecting random actions at each state, transitioning according to the environment dynamics, and accumulating discounted rewards. By averaging the returns from multiple rollouts starting from a given state $s$, we obtain an empirical estimate of the value function $\\hat{U}(s)$.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "The random rollout algorithm estimates the value function $\\hat{U}(s)$ through Monte Carlo sampling:\n",
    "\n",
    "__Initialize__: Given an MDP with state space $\\mathcal{S}$, action space $\\mathcal{A}$, reward function $R(s,a)$, discount factor $\\gamma$, maximum depth $d$, and number of rollouts $N$, initialize the value estimates $\\hat{U}(s) \\gets 0$ and visit counts $n(s) \\gets 0$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "For $i = 1$ to $N$ __do__:\n",
    "\n",
    "1. Initialize the rollout:\n",
    "    - Set starting state $s_{0}$ (chosen uniformly or from a start distribution).\n",
    "    - Set depth counter $t\\gets 0$, cumulative return $G\\gets 0$, and visited states $\\mathcal{V}\\gets\\{s_{0}\\}$.\n",
    "    - Set current state $s \\gets s_{0}$ and $\\texttt{terminated}\\gets\\texttt{false}$.\n",
    "\n",
    "2. While $\\texttt{terminated}$ is $\\texttt{false}$ __do__:\n",
    "    - Select action $a$ uniformly at random from $\\mathcal{A}_{s}$.\n",
    "    - Observe reward $r = R(s, a)$ and update cumulative return: $G \\gets G + \\gamma^{t} \\cdot r$.\n",
    "    - Execute action $a$ and observe next state $s^{\\prime}$.\n",
    "    - Update depth: $t \\gets t + 1$.\n",
    "    - Check termination conditions:\n",
    "        - If $t \\geq d$ (maximum depth reached), set $\\texttt{terminated}\\gets\\texttt{true}$.\n",
    "        - If $s^{\\prime} \\in \\mathcal{V}$ (cycle detected), set $\\texttt{terminated}\\gets\\texttt{true}$.\n",
    "        - If $s^{\\prime}$ is an absorbing state (terminal state), set $\\texttt{terminated}\\gets\\texttt{true}$.\n",
    "        - Otherwise, update $\\mathcal{V} \\gets \\mathcal{V} \\cup \\{s^{\\prime}\\}$ and $s\\gets s^{\\prime}$.\n",
    "\n",
    "3. Update value estimates:\n",
    "    - Increment visit count: $n(s_{0}) \\gets n(s_{0}) + 1$.\n",
    "    - Update value estimate using incremental mean:\n",
    "       $$\\hat{U}(s_{0}) \\gets \\hat{U}(s_{0}) + \\frac{1}{n(s_{0})}\\left(G - \\hat{U}(s_{0})\\right)$$\n",
    "\n",
    "__Output__: The estimated value function $\\hat{U}(s)$ for all states visited during the $N$ rollouts.\n",
    "\n",
    "#### Convergence Properties\n",
    "\n",
    "Random rollout converges to the true value function $U(s)$ as $N \\to \\infty$, without requiring knowledge of the transition model. By averaging empirical returns from multiple rollouts, the sample mean converges to the expected cumulative discounted reward. \n",
    "\n",
    "This model-free property forms the foundation of Monte Carlo methods in reinforcement learning, enabling agents to learn from direct experience in stochastic environments.\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "Random rollout requires $O(N \\cdot d \\cdot |\\mathcal{A}|)$ operations, where $N$ is the number of rollouts, $d$ is the maximum depth per rollout, and $|\\mathcal{A}|$ represents the cost of selecting and executing actions. Unlike value iteration, the complexity does not depend on the state space size $|\\mathcal{S}|$, making random rollout particularly attractive for large or continuous state spaces.\n",
    "\n",
    "The variance of the value estimates decreases as $O(1/\\sqrt{N})$, meaning accuracy improves slowly with additional samples. However, the algorithm can provide useful estimates with relatively few rollouts and scales naturally to high-dimensional problems where model-based approaches become intractable.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff735d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we explored how Markov Decision Processes extend Markov chains by adding actions and rewards to enable optimal sequential decision-making:\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **MDPs add agency to Markov chains**: By introducing actions that influence state transitions and generate rewards, MDPs transform passive stochastic processes into active decision-making frameworks. The discount factor controls how much we value future rewards relative to immediate ones, while policies define how agents choose actions based on current states.\n",
    "> * **Value iteration finds optimal policies through dynamic programming**: The Bellman backup operation iteratively computes the maximum expected cumulative discounted reward for each state by considering all possible actions and their consequences. Convergence guarantees that repeated updates produce the optimal value function and corresponding policy.\n",
    "> * **Random rollout estimates values through sampling**: Monte Carlo simulation generates trajectories by taking random actions and accumulating discounted rewards, providing model-free value estimates that converge as the number of rollouts increases. This sampling approach enables learning from experience without requiring knowledge of transition probabilities.\n",
    "\n",
    "These foundational concepts enable practical applications in robotics, game playing, resource allocation, and autonomous systems where agents must make sequential decisions under uncertainty.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
