{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85886884",
   "metadata": {},
   "source": [
    "# L8c: Regularization and Cross-Validation\n",
    "To prevent overfitting and build robust predictive models, we explore **regularization techniques** that constrain model complexity and **cross-validation methods** that enable principled hyperparameter selection through rigorous performance evaluation on held-out data.\n",
    "\n",
    "> __Learning Objectives__\n",
    ">\n",
    "> By the end of this lecture, students will be able to:\n",
    "> - **Understand and implement ridge regression** (L2 regularization) for overdetermined systems, interpreting how the regularization parameter shrinks parameter estimates toward zero to prevent overfitting while maintaining the analytical tractability of the ordinary least squares framework.\n",
    "> - **Apply SVD-based solutions for regularized regression**, understanding how the singular value decomposition reveals the shrinkage mechanism and provides superior numerical stability compared to direct matrix inversion methods.\n",
    "> - **Select optimal hyperparameters using k-fold cross-validation**, implementing systematic evaluation procedures that partition data into training and validation folds to estimate generalization performance and identify regularization parameters that minimize prediction error on unseen data.\n",
    "\n",
    "Now let's dive into the mathematical foundations of regularization and see how these methods work in practice. Let's go!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0869629",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Today, we will be using the following example(s) to illustrate key concepts:\n",
    "\n",
    "> [▶ Let's build a linear model of housing prices](CHEME-5800-L8c-Example-OLS-SVD-HousingPriceModel-Fall-2025.ipynb). In this example, students will build a linear regression model to predict housing prices based on various features. This will help us understand how to apply ordinary and regularized least squares methods in a practical context.\n",
    "\n",
    "> [▶ Let's play around with the regularization parameter in our housing example](CHEME-5800-L8c-Example-RLS-SVD-HousingPriceModel-Fall-2025.ipynb). In this example, we will compute a linear regression model of house price and explore the impact of regularization on the estimated parameter values, and model performance on unseen test data.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82716401",
   "metadata": {},
   "source": [
    "## Concept Review: Overdetermined Linear Regression\n",
    "Suppose there exists a dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{i},y_{i}) \\mid i = 1,2,\\dots,n\\right\\}$ with $n$ training (labeled) examples, where $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is an $m$-dimensional vector of features (independent input variables) and $y_{i}\\in\\mathbb{R}$ denotes a scalar response variable (dependent variable).\n",
    "\n",
    "We can rewrite the linear regression model in matrix-vector form as:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{y} = \\hat{\\mathbf{X}}\\;\\mathbf{\\theta} + \\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\hat{\\mathbf{X}}$ is an $n\\times{p}$ matrix with the augmented features $\\hat{\\mathbf{x}}_{i}^{\\top}$ on the rows, the target (output) vector $\\mathbf{y}$ is an $n\\times{1}$ column vector with entries $y_{i}$, and the error vector $\\mathbf{\\epsilon}$ is an $n\\times{1}$ column vector with entries $\\epsilon_{i}$. \n",
    "\n",
    "Given a data matrix $\\hat{\\mathbf{X}}\\in\\mathbb{R}^{n\\times{p}}$ that is $\\texttt{overdetermined}$, i.e., $n \\gg p$, and an error model $\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\;\\mathbf{I})$ that follows [a normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with a mean of zero and variance $\\sigma^{2}$, we estimate the model parameters by minimizing the sum of squared errors between the model's estimated outputs and the observed outputs:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}} = \\arg\\min_{\\mathbf{\\theta}} \\frac{1}{2}\\;\\lVert~\\mathbf{y} - \\hat{\\mathbf{X}}\\;\\mathbf{\\theta}~\\rVert^{2}_{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\lVert\\star\\rVert^{2}_{2}$ is the square of the [L2 vector norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm), and $\\hat{\\mathbf{\\theta}}\\in\\mathbb{R}^{p}$ is the estimated parameter vector. When $\\hat{\\mathbf{X}}$ has full column rank (i.e., $\\text{rank}(\\hat{\\mathbf{X}}) = p$ and $\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}$ is invertible), this problem has the analytical solution:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}} &= \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{y}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The confidence intervals for the estimated parameters can be computed as follows:\n",
    "> __Confidence Intervals:__ A $(1-\\alpha) \\times 100\\%$ confidence interval for each parameter $\\hat{\\theta}_j$ is given by:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\hat{\\theta}_j \\pm t_{1-\\alpha/2,\\nu}\\; \\text{SE}(\\hat{\\theta}_j)\n",
    "\\end{align*}\n",
    "$$\n",
    "> where $t_{1-\\alpha/2,\\nu}$ is the $(1-\\alpha/2)$-quantile of a Student $t$ distribution with $\\nu$ degrees of freedom. For a 95% confidence interval, $\\alpha = 0.05$ and $t_{1-\\alpha/2,\\nu} \\approx 1.96$. For a 99.9% confidence interval, $\\alpha = 0.001$ and $t_{1-\\alpha/2,\\nu} \\approx 3.291$. The standard error $\\text{SE}(\\hat{\\theta}_j)$ quantifies the uncertainty in the parameter estimate $\\hat{\\theta}_j$ and is given by:\n",
    ">$$\n",
    "\\begin{align*}\n",
    "\\text{SE}(\\hat{\\theta}_{j}) &= \\hat{\\sigma}\\; \\sqrt{\\bigl[(\\hat{\\mathbf X}^\\top\\hat{\\mathbf X})^{-1}\\bigr]_{jj}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's finish the housing price model example from last time. \n",
    "\n",
    "\n",
    "> __Example__\n",
    ">\n",
    "> [▶ Let's build a linear model of housing prices](CHEME-5800-L8c-Example-OLS-SVD-HousingPriceModel-Fall-2025.ipynb). In this example, students will build a linear regression model to predict housing prices based on various features. This will help us understand how to apply ordinary and regularized least squares methods in a practical context.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3133804",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "In the __overdetermined case__, we can add a regularization term to the objective function to prevent overfitting and improve generalization. This approach is called __regularized linear regression__.\n",
    "\n",
    "> __What is overfitting?__ Overfitting occurs when a model learns the noise in the training data instead of the underlying pattern. This leads to poor performance on unseen data because the model fails to generalize beyond the training set.\n",
    "\n",
    "There are several types of regularization techniques, but we will focus on __Ridge regression__ (also known as Tikhonov regularization or L2 regularization). The ridge regression problem is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} = \\arg\\min_{\\mathbf{\\theta}}\\left( \\frac{1}{2}\\;\\lVert~\\mathbf{y} - \\hat{\\mathbf{X}}\\;\\mathbf{\\theta}~\\rVert^{2}_{2} + \\frac{\\delta}{2}\\;\\lVert~\\mathbf{\\theta}~\\rVert^{2}_{2}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\delta> 0$ is the regularization parameter controlling regularization strength. The first term measures the sum of squared errors, while the second term penalizes large parameter values. The analytical solution for the optimal parameters is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} &= \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{y}\n",
    "\\end{align*}\n",
    "$$\n",
    "This solution can also be expressed in terms of the error model $\\mathbf{\\epsilon}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} &= \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}(\\hat{\\mathbf{X}}\\;\\mathbf{\\theta} + \\mathbf{\\epsilon}) \\\\\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} &= \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}\\;\\mathbf{\\theta} + \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{\\epsilon} \\\\\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} &= \\underbrace{\\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}}_{\\text{Shrinkage}\\;\\mathbf{P}}\\;\\mathbf{\\theta} + \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{\\epsilon} \\\\\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} &= \\mathbf{P}\\;\\mathbf{\\theta} + \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{\\epsilon}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "__Key insight__: The regularization term $\\delta\\;\\mathbf{I}$ acts as a penalty for large parameter values, effectively shrinking the estimated parameters toward zero. This helps prevent overfitting by discouraging complex models that fit the training data too closely.\n",
    "\n",
    "### Uncertainty quantification\n",
    "The $(1-\\alpha) \\times 100\\%$ confidence interval for each regularized parameter $\\hat{\\theta}_{\\delta,j}$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\theta}_{\\delta,j} \\pm t_{1-\\alpha/2,\\nu}\\; \\text{SE}(\\hat{\\theta}_{\\delta,j})\n",
    "\\end{align*}\n",
    "$$\n",
    "We know the standard error for the case without regularization. However, with regularization, the standard error changes. Let's derive the new standard error. The variance of the estimated parameters in ridge regression can be computed as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(\\hat{\\mathbf{\\theta}}_{\\delta}) &= \\sigma^{2}\\; \\underbrace{\\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}\\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}}_{ =\\;\\mathbf{A}_{\\delta}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "The standard errors of the estimated parameters can be computed as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{SE}(\\hat{\\theta}_{\\delta,j}) &= \\hat{\\sigma}\\; \\sqrt{\\bigl[\\mathbf{A}_{\\delta}\\bigr]_{jj}}\\\\\n",
    "& = \\hat{\\sigma}\\; \\sqrt{\\Bigl[\\; \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}\\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\;\\mathbf{I}\\right)^{-1}\\Bigr]_{jj}}\n",
    "\\end{align*}\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db89a95",
   "metadata": {},
   "source": [
    "## SVD solution for Regularized Least Squares\n",
    "Let the singular value decomposition (SVD) of the $n\\times{p}$ data matrix $\\hat{\\mathbf{X}}$ be given by:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{X}} = \\mathbf{U}\\;\\mathbf{\\Sigma}\\;\\mathbf{V}^{\\top}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix, $\\mathbf{\\Sigma} \\in \\mathbb{R}^{n \\times p}$ is a rectangular matrix with singular values on the diagonal,\n",
    "and $\\mathbf{V} \\in \\mathbb{R}^{p \\times p}$ is an orthogonal matrix. The regularized least-squares estimate (Ridge regression) of the unknown parameter vector $\\mathbf{\\theta}$ is given by:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} = \\mathbf{V}\\left(\\mathbf{\\Sigma}^{\\top}\\mathbf{\\Sigma}+\\delta\\mathbf{I}\\right)^{-1}\\mathbf{\\Sigma}^{\\top}\\mathbf{U}^{\\top}\\mathbf{y}\n",
    "\\end{equation*}\n",
    "$$\n",
    "or equivalently, in index notation:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\delta} = \\sum_{i=1}^{r_{\\hat{X}}}\\left(\\frac{\\sigma_{i}}{\\sigma_{i}^{2}+\\delta}\\right)\\left(\\mathbf{u}_{i}^{\\top}\\mathbf{y}\\right)\\mathbf{v}_{i}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $r_{\\hat{X}} = \\min(n,p)$ is the rank of the data matrix $\\hat{\\mathbf{X}}$, $\\mathbf{u}_{i}$ and $\\mathbf{v}_{i}$ are the $i$-th columns of $\\mathbf{U}$ and $\\mathbf{V}$, respectively,\n",
    "$\\sigma_{i}$ is the $i$-th singular value (with $\\sigma_i > 0$), and $\\delta \\geq 0$ is the regularization parameter.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "* **Shrinkage effect**: The regularization parameter $\\delta$ shrinks the contribution of each singular value by the factor $\\frac{\\sigma_{i}}{\\sigma_{i}^{2}+\\delta}$, with smaller singular values being shrunk more aggressively.\n",
    "* **Numerical stability**: This SVD formulation is more numerically stable than directly computing $(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\mathbf{I})^{-1}$, especially when $\\hat{\\mathbf{X}}$ is ill-conditioned.\n",
    "* **Relationship to filtering**: When $\\delta = 0$, we recover the unregularized solution, and as $\\delta \\to \\infty$, all coefficients shrink to zero.\n",
    "\n",
    "Let's revisit our housing example, but in the context of regularization and SVD.\n",
    "\n",
    "> __Example__\n",
    ">\n",
    "> [▶ Let's play around with the regularization parameter in our housing example](CHEME-5800-L8c-Example-RLS-SVD-HousingPriceModel-Fall-2025.ipynb). In this example, we will compute a linear regression model of house price and explore the impact of regularization on the estimated parameter values, and model performance on unseen test data.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a739b0",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Now, the question is: how do we select the regularization parameter $\\delta$? The answer is that we can use cross-validation techniques to tune the hyperparameter $\\delta$ by evaluating the model's performance on held-out validation data.\n",
    "\n",
    "__Cross-Validation (CV)__ is a statistical method used to estimate the performance of machine learning models. It is primarily used in scenarios where the goal is to assess how the results of a statistical analysis will generalize to an independent dataset. \n",
    "\n",
    "> __k-fold cross-validation__: The most common form of cross-validation is k-fold cross-validation, where the dataset is divided into k subsets (or \"folds\"). The model is trained on k-1 folds and validated on the remaining fold. This process is repeated k times, with each fold serving as the validation set once. \n",
    "\n",
    "The performance metrics from each iteration are then averaged to provide a more robust estimate of the model's generalization performance.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db6816",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In lab `L8d` we will implement ridge regression using the SVD approach and apply cross-validation to select the optimal regularization parameter $\\delta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85085b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lecture, we've explored regularization techniques and cross-validation methods that are essential for building robust, generalizable machine learning models:\n",
    "\n",
    "> __Key takeaways:__\n",
    ">\n",
    "> 1. **Ridge regression prevents overfitting through L2 regularization**: The regularized objective $\\hat{\\mathbf{\\theta}}_{\\delta} = \\arg\\min_{\\mathbf{\\theta}}\\left( \\frac{1}{2}\\lVert\\mathbf{y} - \\hat{\\mathbf{X}}\\mathbf{\\theta}\\rVert^{2}_{2} + \\frac{\\delta}{2}\\lVert\\mathbf{\\theta}\\rVert^{2}_{2}\\right)$ yields the analytical solution $\\hat{\\mathbf{\\theta}}_{\\delta} = (\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\mathbf{I})^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{y}$, where the regularization parameter $\\delta > 0$ shrinks parameter estimates toward zero, reducing model complexity and improving generalization to unseen data.\n",
    "> 2. **SVD reveals the shrinkage mechanism and ensures numerical stability**: The singular value decomposition yields $\\hat{\\mathbf{\\theta}}_{\\delta} = \\sum_{i=1}^{r}\\left(\\frac{\\sigma_{i}}{\\sigma_{i}^{2}+\\delta}\\right)(\\mathbf{u}_{i}^{\\top}\\mathbf{y})\\mathbf{v}_{i}$, explicitly showing how regularization dampens contributions from small singular values (which amplify noise) while providing superior numerical stability compared to computing $(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}} + \\delta\\mathbf{I})^{-1}$ directly, especially for ill-conditioned matrices.\n",
    "> 3. **Cross-validation enables principled hyperparameter selection**: K-fold cross-validation systematically partitions the dataset into k subsets, trains on k-1 folds and validates on the held-out fold, repeating k times with each fold serving as validation once. Averaging performance metrics across folds provides a robust estimate of generalization performance, enabling data-driven selection of the optimal $\\delta$ that minimizes prediction error on unseen data.\n",
    ">\n",
    "\n",
    "These techniques form the foundation for practical machine learning applications where preventing overfitting and selecting appropriate model complexity are critical for reliable predictions.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
